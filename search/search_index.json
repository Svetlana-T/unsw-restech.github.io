{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Help and Support","text":""},{"location":"#scheduled-maintenance","title":"Scheduled Maintenance","text":"<p>Febuary 2023 upgrade completed - you may need to complete some steps to reconnect, see FAQ.</p>"},{"location":"#contact-the-research-technology-services-team","title":"Contact the Research Technology Services team","text":"<p>For Katana issues including: functional issues, software installation, reference data sets, general questions -  email the IT Service Centre, including the word Katana in the subject line.</p> <p>Info</p> <p>This is the best and primary way to get help from UNSW Research Technology Services beyond this document. You must use your UNSW email address or your zID. Without this information, we have no idea who you might be.</p> <p>When writing your email, please include a clear and detailed description of the issue experienced, including error messages and node name. Something like \"It doesn't work\" doesn't help us help you! If at all possible, include the steps someone else needs to do to reproduce the problem, the job identifier, the date and time of your problem and on which Katana node it occurred, the script filename and the directory you were running from.</p> <p>Example of a bad request     i'm trying to do some work on katana, but it seems that the server is slow or not responsive at times. i'm logged in from inside unsw today, so working from home shouldn't be the issue.</p> <p>Example of a great request     When I tried to run Sentaurus TCAD today (2020-05-01) on Katana I got this error message regardless of structures I wanted to simulate:</p> <pre><code>\u201cJob failed\nError: Child process with pid '116643' got the signal 'SIGSEGV' (segmentation violation)\njob exits with status 1\u201d\n\nMy job ran on k052 with jobid 300000, my zID is z2134567\n</code></pre> <p>For face to face support: Drop-In Hour Wednesdays, 1pm.</p> <p>For questions about research data at UNSW - on storage, movement or Data Management Plans, please email the Research Data Team. If you specifically wish to increase Katana storage allocations, please email the IT Service Centre - note that such increases are not automatic.</p>"},{"location":"#katana-system-status-and-known-issues","title":"Katana System Status and Known Issues","text":"<p>No known issues at the moment.</p>"},{"location":"#katana-terms-of-use","title":"Katana Terms of Use","text":"<p>Any use of Katana is covered by the Conditions of Use - UNSW ICT Resources. </p> <p>Warning</p> <p>Katana is not suitable for highly sensitive data. You should use the UNSW Data Classification scheme to classify your data and learn about managing your research data by visiting the Research Data Management Hub.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#general-faq","title":"General FAQ","text":""},{"location":"faq/#how-do-i-fix-the-ssh-warning-after-the-february-2023-upgrade","title":"How do I fix the SSH WARNING after the February 2023 upgrade?","text":"<p>Katana's host key has been changed, so reconnecting will trigger a long \"WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!\" message. To fix this, you will need to purge the old known host entries from each of your client devices.</p> <p>Info</p> Command line (Windows/Linux/Mac)Putty (Windows) <ol> <li>Run the \"remove with\" suggestion from your warning, or fix all at once using: <pre><code>ssh-keygen -R katana.unsw.edu.au\nssh-keygen -R kdm.restech.unsw.edu.au\nssh-keygen -R katana.restech.unsw.edu.au\nssh-keygen -R katana1.restech.unsw.edu.au\nssh-keygen -R katana2.restech.unsw.edu.au\nssh-keygen -R katana3.restech.unsw.edu.au\n</code></pre></li> </ol> <ol> <li>Search and open regedit.exe</li> <li>Navigate to HKEY_CURRENT_USER/SOFTWARE/SimonTatham/PuTTy/SshHostKeys</li> <li>Right click on any katana/kdm keys and click delete</li> </ol> <p>Adapted from 1.</p>"},{"location":"faq/#where-is-the-best-place-to-store-my-code","title":"Where is the best place to store my code?","text":"<p>The best place to store source code is to use version control and store it in a repository.  This means that you will be able to keep every version of your code and revert to an earlier version if you require. UNSW has a central github account, but we encourage you to create your own.</p>"},{"location":"faq/#i-just-got-some-money-from-a-grant-what-can-i-spend-it-on","title":"I just got some money from a grant. What can I spend it on?","text":"<p>There are a number of different options for using research funding to improve your ability to run computationally intensive programs. The best starting point is to contact us to figure out the different options.  </p>"},{"location":"faq/#can-i-access-katana-from-outside-unsw","title":"Can I access Katana from outside UNSW?","text":"<p>Yes, if you have an account then you can connect to Katana from both inside and outside UNSW. Some services - like remote desktops - will not be as responsive as inside the UNSW network.</p>"},{"location":"faq/#scheduler-faq","title":"Scheduler FAQ","text":""},{"location":"faq/#does-katana-run-a-32-bit-or-a-64-bit-operating-system","title":"Does Katana run a 32 bit or a 64 bit operating system?","text":"<p>Katana Compute Nodes run a 64 bit version of the CentOS distribution of Linux. Currently version 7.8. The Head Node runs RedHat 7.8.</p>"},{"location":"faq/#how-much-memory-is-available-per-core-andor-per-node","title":"How much memory is available per core and/or per node?","text":"<p>The amount of memory available varies across the cluster. To determine how much memory each node has available use the 'pbsnodes' command. Roughly, you can safely use 4GB per core requested. You can request more memory but it may delay time spent in the queue.</p>"},{"location":"faq/#how-much-memory-can-i-use-on-the-login-node-for-compiling-software","title":"How much memory can I use on the login node for compiling software?","text":"<p>The login nodes have a total of 24GB of memory each. Each individual user is limited to 4GB and should only be used to compile software. If you need more, do it in an Interactive Job.</p>"},{"location":"faq/#why-isnt-my-job-making-it-onto-a-node-even-though-it-says-that-some-nodes-are-free","title":"Why isn't my job making it onto a node even though it says that some nodes are free?","text":"<p>There are three main reasons you will see this behavior. The first of them is specific to Katana and the other two apply to any cluster.</p> <p>Firstly, the compute nodes in Katana belong to various schools and research groups across UNSW. Any job with an expected run-time longer than 12 hours can only run on a compute node that is somehow associated with the owner of the job. For example, if you are in the CCRC you are entitled to run 12+ hour jobs on the General nodes and the nodes jointly purchased by CCRC. However, you cannot run 12+ hour jobs on the nodes purchased by Astrobiology, Statistics, TARS, CEPAR or Physics. So you may see idle nodes, but you may not be entitled to run a 12+ hour job on them.</p> <p>Secondly, the idle nodes may not have sufficient resources for your job. For example, if you have asked for 100GB memory but there are only 50GB free on the \"idle node\".</p> <p>Thirdly, there may be distributed memory jobs ahead of your job in the queue which have reservations on the idle nodes, and they are just waiting for all of their requested resources to become available. In this case, your job can only use the reserved nodes if your job can finish before the nodes are required by the distributed memory job. For example, if a job has been waiting a week (yes, it happens) for <code>walltime=200,cpu=88,mem=600GB</code> (very long, two whole nodes), then those resources will need to be made available at some point. This is an excellent example of why breaking your jobs up into smaller parts is good HPC practice.</p>"},{"location":"faq/#how-many-jobs-can-i-submit-at-the-one-time","title":"How many jobs can I submit at the one time?","text":"<p>Technically you can submit as many jobs as you wish. The queuing system run by the scheduler is designed to prevent a single user flooding the system - each job will reduce the priority of your next jobs. In this way the infrequent users get a responsive system without impacting the regular users too much.</p> <p>Whilst there is not a technical limit to the number of jobs you can submit, submitting more that 2,000 jobs at the one time can place an unacceptable load on the job scheduler and your jobs may be deleted without warning. This is an editorial decision by management.</p>"},{"location":"faq/#what-is-the-maximum-number-of-cpus-i-can-use-in-parallel","title":"What is the maximum number of CPUs I can use in parallel?","text":"<p>As many as your account and queue will allow you. But there are trade-offs - if you ask for 150 CPUs (~5 full servers) you might be waiting more than a couple of months for your job to run. </p> <p>If you are regularly wanting to run large parallel jobs (16+ cores per job) on Katana you should consider seeking support so that we are aware of your jobs. We may be able to provide you additional assistance on resource usage for parallel jobs. </p>"},{"location":"faq/#why-does-my-ssh-connection-periodically-disconnect","title":"Why does my SSH connection periodically disconnect?","text":"<p>With all networks there is a limit to how long a connection between two computers will stay open if no data is travelling between them. Look to set your ServerAliveInterval or Keep Alive interval to 60 in your secure shell software (putty, ssh). </p>"},{"location":"faq/#can-i-change-the-job-script-after-it-has-been-submitted","title":"Can I change the job script after it has been submitted?","text":"<p>Yes you increase the resource values for jobs that are still queued, but even then you are constrained by the limits of the particular queue that you are submitting to. Once it has been assigned to a node the intricacies of the scheduling policy means that it becomes impossible for anyone including the administrator to make any further changes</p>"},{"location":"faq/#where-does-standard-output-stdout-go-when-a-job-is-run","title":"Where does Standard Output (STDOUT) go when a job is run?","text":"<p>By default Standard Output is redirected to storage on the node and then transferred when the job is completed. If you are generating data you should redirect<code>STDOUT</code> to a different location. The best location depends on the characteristics of your job but in general all<code>STDOUT</code> should be redirected to local scratch.</p>"},{"location":"faq/#how-do-i-figure-out-what-the-resource-requirements-of-my-job-are","title":"How do I figure out what the resource requirements of my job are?","text":"<p>The best way to determine the resource requirements of your job is to be generous with the resource requirements on the first run and then refine the requirements based on what the job actually used. If you put the following information in your job script you will receive an email when the job finishes which will include a summary of the resources used.</p> <pre><code>    #PBS -M z1234567@unsw.edu.au \n#PBS -m ae\n</code></pre>"},{"location":"faq/#can-i-cause-problems-to-other-users-if-i-request-too-many-resources-or-make-a-mistake-with-my-job-script","title":"Can I cause problems to other users if I request too many resources or make a mistake with my job script?","text":"<p>Yes, but it's extremely unlikely. We used to say no, but that's not strictly true. The reality is that if something breaks it's usually your job hitting the odd corner case we didn't account for. It doesn't happen often.</p>"},{"location":"faq/#will-a-job-script-from-another-cluster-work-on-cluster-x","title":"Will a job script from another cluster work on cluster X?","text":"<p>It depends on a number of factors including the sceduling software. Some aspects are fairly common across different clusters (e.g. walltime) others are not. You should look at the cluster specific information to see what queuing system is being used on that cluster and what commands you will need to change. You won't find a cluster that doesn't have knowledgable support that can help you migrate.</p>"},{"location":"faq/#how-can-i-see-exactly-what-resources-io-cpu-memory-and-scratch-my-job-is-currently-using","title":"How can I see exactly what resources (I/O, CPU, memory and scratch) my job is currently using?","text":"<p>From outside the job, you can run<code>qstat -f &lt;jobid&gt;</code>. </p> <p>If, for instance, you wanted to measure different steps of your process, then inside your jobscript you can put<code>qstat -f $PBS_JOBID</code></p> <p>For fine grain detail, you may need to get access to the worker node that the job is running on:</p> <pre><code>    qstat -nru $USER\n</code></pre> <p>then you can see a list of your running jobs and where they are running. You can then use ssh to log on to the individual nodes and run<code>top</code> or<code>htop</code> to see the load on the node including memory usage for each of the processes on the node.</p>"},{"location":"faq/#how-do-i-request-the-installation-or-upgrade-of-a-piece-of-software","title":"How do I request the installation or upgrade of a piece of software ?","text":"<p>If you wish to have a new piece of software installed or software that is already installed upgraded please send an email to the IT Service Centre from your UNSW email account with details of what software change you require.</p>"},{"location":"faq/#why-is-my-job-stuck-in-the-queue-whilst-other-jobs-run","title":"Why is my job stuck in the queue whilst other jobs run?","text":"<p>The queues are not set up to be first-in-first-out. In fact all of the queued jobs sit in one big pool of jobs that are ready to run. The scheduler assigns priorities to jobs in the pool and the job with the highest priority is the next one to run. The length of time spent waiting in the pool is just one of several factors that are used to determine priority.</p> <p>For example, people who have used the cluster heavily over the last two weeks receive a negative contribution to their jobs' priority, whereas a light user will receive a positive contribution. You can see this in action with the diagnose -p and diagnose -f commands.</p>"},{"location":"faq/#you-mentioned-waiting-time-as-a-factor-what-else-affects-the-job-priority","title":"You mentioned waiting time as a factor, what else affects the job priority?","text":"<p>The following three factors combine to generate the job priority.</p> <ul> <li>How many resources (cpu and memory) have you and your group consumed in the last 14 days? Your personal consumption is weighted more highly than your group's consumption. Heavy recent usage contributes a negative priority. Light recent usage contributes a positive priority.</li> <li>How many resources does the job require? Always a positive contribution to priority, but increases linearly with the amount of cpu and memory requested, i.e. we like big jobs.</li> <li>How long has the job been waiting in the queue? Always a positive contribution to priority, but increases linearly with the amount of time your job has been waiting in the queue. Note that throttling policies will prevent some jobs from being considered for scheduling, in which case their clock does not start ticking until that throttling constraint is lifted.</li> </ul>"},{"location":"faq/#what-happens-if-my-job-uses-more-memory-than-i-requested","title":"What happens if my job uses more memory than I requested?","text":"<p>The job will be killed by the scheduler. You will get a message to that effect if you have any types of notification enabled (logs, emails).</p>"},{"location":"faq/#what-happens-if-my-job-is-still-running-when-it-reaches-the-end-of-the-time-that-i-have-requested","title":"What happens if my job is still running when it reaches the end of the time that I have requested?","text":"<p>When your job hits its Walltime it is automatically terminated by the scheduler.</p>"},{"location":"faq/#200-hours-is-not-long-enough-what-can-i-do","title":"200 hours is not long enough! What can I do?","text":"<p>If you find that your jobs take longer than the maximum WALL time then there are several different options to change your code so that it fits inside the parameters.</p> <ul> <li>Can your job be split into several independent jobs?</li> <li>Can you export the results to a file which can then be used as input for the next time the job is run?</li> </ul> <p>You may want to also look to see if there is anything that you can do to make your code run better like making better use of local scratch if your code is I/O intensive.</p>"},{"location":"faq/#do-sub-jobs-within-an-array-job-run-in-parallel-or-do-they-queue-up-serially","title":"Do sub-jobs within an array job run in parallel, or do they queue up serially?","text":"<p>Submitting an array job with 100 sub-jobs is equivalent to submitting 100 individual jobs. So if sufficient resources are available then all 100 sub-jobs could run in parallel. Otherwise some sub-jobs will run and other sub-jobs must wait in the queue for resources to become available.</p> <p>The '%' option in the array request offers the ability to self impose a limit on the number of concurrently running sub-jobs. Also, if you need to impose an order on when the jobs are run then the 'depend' attribute can help.</p>"},{"location":"faq/#in-a-pbs-file-does-the-mem-requested-refer-to-each-node-or-the-total-memory-on-all-nodes-being-used-if-i-am-using-more-than-1-node","title":"In a pbs file does the MEM requested refer to each node or the total memory on all nodes being used (if I am using more than 1 node?)","text":"<p>MEM refers to the amount of memory per node.</p>"},{"location":"faq/#storage-faq","title":"Storage FAQ","text":""},{"location":"faq/#what-storage-is-available-to-me","title":"What storage is available to me?","text":"<p>Katana provides three different storage areas, cluster home drives, local scratch and global scratch. The storage page has additional information on the differences and advantages of each of the different types of storage. You may also want to consider storing your code using a version control service like GitHub. This means that you will be able to keep every version of your code and revert to an earlier version if you require.</p>"},{"location":"faq/#which-storage-is-fastest","title":"Which storage is fastest?","text":"<p>In order of performance the best storage to use is local scratch, global scratch and cluster home drive.</p>"},{"location":"faq/#is-any-of-the-cluster-based-storage-backed-up","title":"Is any of the cluster based storage backed up?","text":"<p>The only cluster based storage that gets backed up is the cluster home drives. All other storage including local and global scratch is not backed up.</p>"},{"location":"faq/#how-do-i-actually-use-local-scratch","title":"How do I actually use local scratch?","text":"<p>The easiest way of making use of local scratch is to use scripts to copy files to the node at the start of your job and from the node when your job finishes. You should also use local scratch for your working directory and temporary files.</p>"},{"location":"faq/#why-am-i-having-trouble-creating-a-symbolic-link","title":"Why am I having trouble creating a symbolic link?","text":"<p>Not all filesystems support symbolic links. The most common examples are some Windows network shares. On Katana this includes Windows network shares such as hdrive. The target of the symbolic link can be within such a filesystem, but the link itself must be on a filesystem that supports symbolic links, e.g. the rest of your home directory or your scratch directory. </p>"},{"location":"faq/#what-storage-is-available-on-compute-nodes","title":"What storage is available on compute nodes?","text":"<p>As well as local scratch, global scratch and your cluster home drive are accessible on the compute nodes.</p>"},{"location":"faq/#what-is-the-best-way-to-transfer-a-large-amount-of-data-onto-a-cluster","title":"What is the best way to transfer a large amount of data onto a cluster?","text":"<p>Use<code>rsync</code> to copy data to the KDM server. More information is above.</p>"},{"location":"faq/#is-there-any-way-of-connecting-my-own-file-storage-to-one-of-the-clusters","title":"Is there any way of connecting my own file storage to one of the clusters?","text":"<p>Whilst it is not possible to connect individual drives to any of the clusters, some units and research groups have purchased large capacity storage units which are co-located with the clusters. This storage is then available on the cluster nodes. For more information please contact the Research Technology Service Team by placing a request with the IT Service Centre.</p>"},{"location":"faq/#can-i-specify-how-much-file-storage-i-want-on-local-scratch","title":"Can I specify how much file storage I want on local scratch?","text":"<p>If you want to specify the minimum amount of space on the drive before your job will be assigned to a node then you can use the file option in your job script. Unfortunately setting up more complicated file requirements is currently problematic.</p>"},{"location":"faq/#can-i-run-a-program-directly-from-scratch-or-my-home-drive-after-logging-in-to-the-cluster-rather-submitting-a-job","title":"Can I run a program directly from scratch or my home drive after logging in to the cluster rather submitting a job?","text":"<p>As the file server does not have any computational resources you would be running the job from the head node on the cluster. If you need to enter information when running your job then you should start an interactive job.</p>"},{"location":"faq/#expanding-katana","title":"Expanding Katana","text":"<p>Katana has significant potential for further expansion. It offers a simple and cost-effective way for research groups to invest in a powerful computing facility and take advantage of the economies that come with joining a system with existing infrastructure. A sophisticated job scheduler ensures that users always receive a fair share of the compute resources that is at least commensurate with their research group\u2019s investment in the cluster. For more information please contact us.</p>"},{"location":"faq/#acknowledging-katana","title":"Acknowledging Katana","text":"<p>If you use Katana for calculations that result in a publication then you should add the following text to your work.</p> <p>This research includes computations using the computational cluster Katana supported by Research Technology Services at UNSW Sydney.</p> <p>Katana now also has a DOI that can be used for citation in papers: https://doi.org/10.26190/669x-a286</p> <p>If you are using nodes that have been purchased using an external funding source you should also acknowledge the source of those funds.</p> <p>For information refer to acknowledging ARC funding</p> <p>Your School or Research Group may also have policies for compute nodes that they have purchased.</p>"},{"location":"faq/#facilities-external-to-unsw","title":"Facilities external to UNSW","text":"<p>If you are using facilities at Intersect_ or NCI_ in addition to Katana they may also require some form of acknowledgement.</p> <p>Intersect NCI</p> <ol> <li> <p>https://www.rit.edu/researchcomputing/instructions/Clearing-the-known_hosts-SSH-File\u00a0\u21a9</p> </li> </ol>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#active-job","title":"Active Job","text":"<p>Active jobs are jobs that have been assigned to a compute node and are currently running. These can be seen by running <code>qstat</code> and looking for an A in the second last column. See examples.</p>"},{"location":"glossary/#array-job","title":"Array Job","text":"<p>If you want to run the same job multiple times with slight differences (filenames, data source, etc), then you can create an array job which will submit multiple jobs for you from the one job script. </p>"},{"location":"glossary/#batch-job","title":"Batch Job","text":"<p>A batch job is a job on a cluster that runs without any further input once it has been submitted. Almost all jobs on the cluster are batch jobs. All jobs are either batch jobs or Interactive Job.</p>"},{"location":"glossary/#blade","title":"Blade","text":"<p>Some of the compute nodes Katana are called blade servers which allow a higher density of servers in the same space. Each blade consists of multiple CPUs with 6 or more cores.</p>"},{"location":"glossary/#cluster","title":"Cluster","text":"<p>A computer cluster is a set of connected computers that work together so that, in many respects, they can be viewed as a single system. Using a cluster is referred to as High Performance Computing or HPC. Most will have a Management Plane and several Compute Nodes.</p>"},{"location":"glossary/#compute-nodes","title":"Compute Nodes","text":"<p>The compute nodes are where the compute jobs run. Users submit jobs from the Login Node and the Job Scheduler on the Head Node will assign the job to one or more compute nodes.</p>"},{"location":"glossary/#cpu-core","title":"CPU Core","text":"<p>Each node in the cluster has one or more CPUs each of which has 6 or more cores. Each core is able to run one job at a time so a node with 12 cores could have 12 jobs running in parallel.</p>"},{"location":"glossary/#data-transfer-node","title":"Data Transfer Node","text":"<p>The Data Transfer Node, also known as the Katana Data Mover (KDM), is a server that is used for transferring files to, from, and within the cluster. Due to the nature of moving data around, it uses a significant amount of memory and network bandwidth. This server is used to take that load off the Login Node.</p>"},{"location":"glossary/#environment-variable","title":"Environment Variable","text":"<p>Environment variables are variables that are set in Linux to tell applications where to find programs and set program options. They will start with a $ symbol. For example, all users can reference <code>$TMPDIR</code> in their Job Script in order to use Local Scratch</p>"},{"location":"glossary/#global-scratch","title":"Global Scratch","text":"<p>Global scratch is a large data store for data that isn't backed up. It differs from local scratch in that it is available from every node including the Head Node. If you have data files or working directories this is where you should put them.</p>"},{"location":"glossary/#head-node","title":"Head Node","text":"<p>The head node of the Cluster is the computer that manages job and resource management. This is where the Job Scheduler and Resource Manager are run. It is kept separate from the Login Node so that production doesn't stop if someone accidentally breaks the Login Node.</p>"},{"location":"glossary/#held-jobs","title":"Held Jobs","text":"<p>Held jobs are jobs that cannot currently run. They are put into that state by either the server or the system administrator. Jobs stay held until released by a systems administrator, at which point they become Queued Jobs. These can be seen by running <code>qstat</code> and looking for an H in the second last column. See examples.</p>"},{"location":"glossary/#interactive-job","title":"Interactive Job","text":"<p>An interactive job is a way of testing your program and data on a cluster without negatively impacting the Login Node. Once a request has been submitted and accepted for an interactive job, the user will no longer be on the relatively small login nodes, and will have access to the resources requested on the Compute Nodes. In other words, your terminal session will move from a small (virtual) computer you share with many people to a large computer you share with very few people. All jobs are either a Batch Job or an interactive job. Instructions on using interactive jobs.</p>"},{"location":"glossary/#job-scheduler","title":"Job Scheduler","text":"<p>The job scheduler monitors the jobs currenty running on the cluster and assigns Queued Jobs to Compute Nodes based on recent cluster useage, job resource requirements and nodes available to the research group of the submitter. In summary the job scheduler determines when and where a job should run. The job scheduler that we use is called PBSPro.</p>"},{"location":"glossary/#job-script","title":"Job Script","text":"<p>A job script is a file containing all of the information needed to run a Batch Job including the resource requirements and the actual commands to run the job.</p>"},{"location":"glossary/#local-scratch","title":"Local Scratch","text":"<p>Local scratch refers to the storage available internally on each compute node. Of all the different scratch directories this storage has the best performance however you will need to move your data into local scratch as part of your job script. You can use local scratch with the Environment Variable <code>$TMPDIR</code></p>"},{"location":"glossary/#login-node","title":"Login Node","text":"<p>The login nodes of the cluster is the computer that you log in to when you connect to the cluster. This node is used to compile software and submit jobs.</p>"},{"location":"glossary/#module","title":"Module","text":"<p>The module command is a means of providing access to different versions of software without risking version conflicts across multiple users.</p>"},{"location":"glossary/#management-plane","title":"Management Plane","text":"<p>The Management Plane is the set of servers that sit above or adjacent to the Compute Nodes. These servers are used to manage the system, manage the storage, or manage the network. Users have access to the Login Node and Data Transfer Node. Other servers include the Head Node. </p>"},{"location":"glossary/#mpi","title":"MPI","text":"<p>Message Passing Infrastructure (MPI) is a technology for running a Batch Job on more than one Compute Nodes. Designed for situations where parts of the job can run on independent nodes with the results being transferred to other nodes for the next part of the job to be run.</p>"},{"location":"glossary/#network-drive","title":"Network Drive","text":"<p>A network drive is a drive that is independent from the cluster. </p>"},{"location":"glossary/#queue","title":"Queue","text":"<p>All submitted jobs are put into a queue. Each queue has a collection of resources available to it. As those resources become available, new jobs will be assigned to those resources. Job prioritisation is done by the scheduler and depends on a number of factors including length of wait time and total resource use by the user over the previous month.</p>"},{"location":"glossary/#queued-jobs","title":"Queued Jobs","text":"<p>Queued jobs are eligible to run but are waiting for a Compute Nodes that matches their requirements to become available. Which idle job will be assigned to a compute node next depends on the Job Scheduler. These can be seen by running <code>qstat</code> and looking for a Q in the second last column. See examples.</p>"},{"location":"glossary/#resource-manager","title":"Resource Manager","text":"<p>A resource manager works with the Job Scheduler to manage running jobs on a cluster. Amongst other tasks it receives and parses job submissions, starts jobs on Compute Nodes, monitors jobs, kills jobs, and manages how many CPU Core are available on each Compute Nodes</p>"},{"location":"glossary/#scratch-space","title":"Scratch Space","text":"<p>Scratch space is a non backed up storage area where users can store transient data. It should not be used for job code as it is not backed up.</p>"},{"location":"glossary/#walltime","title":"Walltime","text":"<p>In HPC, walltime is the amount of time that you will be allocated when your job runs. If your jobs runs longer than the walltime, it will be killed by the Job Scheduler. It is used by the scheduler for helping allocate resources onto servers. On Katana it is also used to determine which Queue your job will end up in. The shorter the walltime, the more opportunity your job has to run which in turn means that it will start sooner. In short it's harder to find 100 hours of aviable time than it is to find 12 hours.</p>"},{"location":"reference_data/","title":"Reference Data","text":"<p>We keep a number of reference data sets available on Katana at <code>/data/</code> so that we don't accidentally - for instance - end up with 150 copies of the Human Genome in user's home directories.</p> <p>As these are reference data, they don't change often and we can update them as necessary.</p> Directory Description Update Schedule URL annovar Reference datasets that come with software installation. Installed when software is installed. annovar.openbioinformatics.org antismash Reference files and commands for antismash version 4.2.0 Version specific database installed when software is installed antismash.secondarymetabolites.org blast NCBI nr, nt, refseq_genomic and refseq datasets Updated on release 6 times a year www.ncbi.nlm.nih.gov/refseq blastv5 Version 5 of NCBI nr, nt, refseq_genomic and refseq datasets.  Updated on release 6 times a year. www.ncbi.nlm.nih.gov/refseq diamond Diamond reference databases for versions 0.8.38, 0.9.10, 0.9.22 and 0.9.24. Database format periodically changes. Updated when NCBI nr databases are updated. ab.inf.uni-tuebingen.de/software/diamond gtdbtk Version specific database installed when software is installed. gtex Genotype-Tissue Expression project, comprehensive resource to study tissue-specific gene expression and regulation Please contact the [Oates lab](mailto:e.oates@unsw.edu.au) for access to a large set of GTEx datathat is not publicly available. https://gtexportal.org/home/ hapcol Reference datasets that come with software installation. Installed when software is installed. hapcol.algolab.eu hg19 Human reference genome hg19 (GRCh37). Fixed reference. Never updated. www.ncbi.nlm.nih.gov/grc interproscan Reference datasets for InterProScan versions 5.20-59.0 and 5.35-74.0 Version specific database installed when software is installed. www.ebi.ac.uk/interpro itasser Rererence datasets for I-TASSER plus link to current nr database. Version specific databases installed when software is installed plus link to nr database (see blast above). zhanglab.ccmb.med.umich.edu/I-TASSER kaiju Reference databases for all versions of Kaiju. Same databases for all versions. Databases installed when software is installed. kaiju.binf.ku.dk matam Reference databases for all MATAM versions. Version specific database installed when software is installed. github.com/bonsai-team/matam megan Reference databases for all MEGAN versions. Version specific database installed when software is installed. ab.inf.uni-tuebingen.de/software/megan6 repeatmasker Reference datasets for RepeatMasker version 4.0.7 Version specific database installed when software is installed. www.repeatmasker.org sra Sequence Read Archive, repository of high throughput sequencing data https://www.ncbi.nlm.nih.gov/sra trinotate Reference databases for all versions of Kaiju. Same databases for all versions. Databases installed when software is installed. trinotate.github.io"},{"location":"software/ansys/","title":"Ansys","text":"<p>Both Ansys Workbench and Ansys Electronic Desktop are available on Katana OnDemand. </p> <p>This is the most user-friendly approach to running Ansys jobs on Katana.</p>"},{"location":"software/ansys/#ansys-batch-jobs","title":"Ansys Batch Jobs","text":"<p>Ansys workbench is available on myAccess for undergraduate and postgraduate students in the Faculty of Engineering. </p> <p>If you install Ansys on your local machine, you can generate your model files locally and transfer them tto katana to run in a batch job. </p>"},{"location":"software/ansys/#ansys-cfx","title":"Ansys CFX","text":"<p>For Ansys CFX the .cfx and .def files need to be transferred to Katana.</p> <p>A brief batch script example is given below. A step-by-step guide is availabe on our GitHub.</p> <pre><code>   #!/bin/bash\n#&lt;RESOURCE REQUESTS&gt;\n\ncd $PBS_O_WORKDIR\n\nmodule load intelmpi/2019.6.166\n   module load ansys/2021r1\n\n   cfx5solve -batch -def &lt;filename&gt;.def -part $NCPUS -start-method \"Intel MPI Local Parallel\"\n</code></pre>"},{"location":"software/ansys/#ansys-fluent","title":"Ansys Fluent","text":"<p>Similarly, Ansys Fluent input can be generated locally and transferred to Katana to run in a batch job.</p> <p>Again, a brief batch script is below, and a step-by-step guide is available on our Github. </p> <pre><code>   #!/bin/bash\n#&lt;RESOURCE REQUESTS&gt;\n\ncd $PBS_O_WORKDIR\n\nmodule load ansys/2021r1\n   module load intelmpi/2019.6.166\n\n   fluent 3d -g -t $NCPUS -i fluent.in &gt; output.out\n</code></pre>"},{"location":"software/ansys/#ansys-on-gadi","title":"Ansys on Gadi","text":"<p>If you wish to use Ansys on NCI's Gadi, UNSW has a institutional licence that requies you join the relevant software group as detailed in NCI's documentation.</p>"},{"location":"software/biosciences/","title":"Biosciences","text":"<p>There are a number of Bioscience softwares installed. Note that if the software you want isn't obvious, it might be within another package:</p>"},{"location":"software/biosciences/#stand-alone","title":"Stand alone","text":"<p>Blast+: <code>module load blast+/2.9.0</code></p> <p>Mothur: <code>module load mothur/1.44.2</code></p>"},{"location":"software/biosciences/#python-module","title":"Python Module","text":"<p><code>module avail python</code></p> <p>BioPython</p> <p>Snakemake</p>"},{"location":"software/biosciences/#r-module","title":"R Module","text":"<p><code>module avail R</code></p> <p>Bioconductor</p>"},{"location":"software/biosciences/#perl-module","title":"Perl Module","text":"<p><code>module avail perl</code></p> <p>BioPerl </p>"},{"location":"software/comsol/","title":"Comsol","text":"<p>Comsol is best run interactively on Katana OnDemand. </p> <p>Note</p> <p>You will need to belong to a group that owns a COMSOL licence (mech, spree, quantum, biomodel) </p>"},{"location":"software/comsol/#comsol-batch-jobs","title":"Comsol Batch Jobs","text":"<p>COMSO is available to download on myAccess for undergraduate and postgraduate students in Chemical and Biomedical Engineering. This will allow you to generate module files locally, and transfer them to Katana to process in a batch job.</p> <p>An example comsol batch job file is available on our GitHub and an uncommented version is reproduced below. </p> console<pre><code>    #!/bin/bash\n#&lt;RESOURCE REQUESTS&gt;\n\nmkdir -p ${TMPDIR}/comsol\n\n    export MY_COMSOL_DIR=/srv/scratch/$USER/comsoldir\n\n    module load comsol/5.6-spree\n\n    comsol -nn 1 -np $NCPUS \\\n-recoverydir ${MY_COMSOL_DIR}/recoveries \\\n-tmpdir ${TMPDIR}/comsol \\\nbatch \\\n-inputfile ${MY_COMSOL_DIR}/MyModel.mph \\\n-outputfile ${MY_COMSOL_DIR}/MyModelOut.mph \\\n-batchlog ${MY_COMSOL_DIR}/MyModel.log\n</code></pre>"},{"location":"software/environment_modules/","title":"Environment Modules","text":"<p>Environment Modules offer a simple means of customising your environment to access the required versions of installed software and this section provides information on how they are used on Katana.</p> <p>When we use modules, we are changing our \"environment\", hence the name. </p>"},{"location":"software/environment_modules/#how-do-i-discover-what-software-is-available","title":"How do I discover what software is available?","text":"<pre><code>[z1234567@katana ~]$ module avail \n\n-------------------------- /share/apps/modules/intel ---------------------------\nintel/11.1.080(default)  intel/12.1.7.367  intel/13.0.1.117  intel/13.1.0.146\n\n--------------------------- /share/apps/modules/pgi ----------------------------\npgi/13.7\n\n-------------------------- /share/apps/modules/matlab --------------------------\nmatlab/2007b          matlab/2010b          matlab/2012a(default)\nmatlab/2008b          matlab/2011a          matlab/2012b\nmatlab/2009b          matlab/2011b          matlab/2013a\n</code></pre>"},{"location":"software/environment_modules/#what-if-the-software-that-i-want-is-not-on-the-list","title":"What if the software that I want is not on the list?","text":"<p>If you require software installed on the cluster, email the IT Service Centre detailing the software that you would like installed and that you would like to have it installed on Katana. Please include links and desired version numbers.</p>"},{"location":"software/environment_modules/#how-do-i-add-a-particular-version-of-software-to-my-environment","title":"How do I add a particular version of software to my environment?","text":"<p><pre><code>[z1234567@katana1 ~]$ module add matlab/2018b\n</code></pre> or</p> <pre><code>[z1234567@katana1 ~]$ module load matlab/2018b\n</code></pre>"},{"location":"software/environment_modules/#how-do-i-remove-a-particular-version-of-software-from-my-environment","title":"How do I remove a particular version of software from my environment?","text":"<pre><code>[z1234567@katana1 ~]$ module rm matlab/2018b\n</code></pre> <p>or</p> <pre><code>[z1234567@katana1 ~]$ module unload matlab/2018b\n</code></pre>"},{"location":"software/environment_modules/#how-do-i-remove-all-modules-from-my-environment","title":"How do I remove all modules from my environment?","text":"<pre><code>[z1234567@katana1 ~]$ module purge\n</code></pre>"},{"location":"software/environment_modules/#which-versions-of-software-am-i-currently-using","title":"Which versions of software am I currently using?","text":"<pre><code>[z1234567@katana1 ~]$ module list\n    Currently Loaded Modulefiles:\n     1) intel/18.0.1.163   2) matlab/2018b\n</code></pre>"},{"location":"software/environment_modules/#how-do-i-find-out-more-about-a-particular-piece-of-software","title":"How do I find out more about a particular piece of software?","text":"<p>You can find out more about a piece of software by using the module help command. For example:</p> <pre><code>[z1234567@katana1 ~]$ module help mrbayes\n\n----------- Module Specific Help for 'mrbayes/3.2.2' --------------\n\nMrBayes 3.2.2 is installed in /apps/mrbayes/3.2.2\n\nThis module was complied against beagle/2.1.2 and openmpi/1.6.4 with MPI support.\n\nMore information about the commands made available by this module is available\nat http://mrbayes.sourceforge.net\n</code></pre>"},{"location":"software/environment_modules/#how-do-i-switch-between-particular-versions-of-software","title":"How do I switch between particular versions of software?","text":"<pre><code>[z1234567@katana1 ~]$ module switch matlab/2018b matlab/2017b\n</code></pre>"},{"location":"software/environment_modules/#how-can-i-find-out-what-paths-and-other-environment-variables-a-module-uses","title":"How can I find out what paths and other environment variables a module uses?","text":"<pre><code>[z1234567@katana1 ~]$ module show mothur/1.42.3\n-------------------------------------------------------------------\n/apps/modules/bio/mothur/1.42.3:\n\nmodule-whatis     Mothur 1.42.3 \nconflict     mothur \nsetenv         MOTHUR_ROOT /apps/mothur/1.42.3 \nprepend-path     PATH /apps/mothur/1.42.3/bin \nsetenv         LAST_MODULE_TYPE bio \nsetenv         LAST_MODULE_NAME mothur/1.42.3 \nsetenv         LAST_MODULE_VERSION 1.42.3 \n-------------------------------------------------------------------\n</code></pre>"},{"location":"software/environment_modules/#why-does-the-cluster-forget-my-choice-of-modules","title":"Why does the cluster forget my choice of modules?","text":"<p>Environment modules only affect the particular session in which they are loaded. Loading a module in one SSH session will not affect any other SSH session or even any jobs submitted from that session. Modules must be loaded in every session where they will be used.</p>"},{"location":"software/environment_modules/#how-can-i-invoke-my-module-commands-automatically","title":"How can I invoke my module commands automatically?","text":"<p>The best way of doing this is to add your Module commands to your job scripts. This approach is useful for preserving the required environment for each job. For example:</p> <pre><code>#!/bin/bash\n\n#PBS -l nodes=1:ppn=1\n#PBS -l vmem=4gb\n#PBS -j oe\n\nmodule purge\nmodule add intel/18.0.1.163\n\ncd ${PBS_O_WORKDIR}\n\n./myprog\n</code></pre> <p>Perl, Python and R all have their own library/module systems - <code>CPAN</code>, <code>PyPI`` and</code>CRAN`. If a library or module you want from one of these sources isn't installed in the module, please email us at IT Service Desk.</p> <p>CPAN</p> <p>PyPI</p> <p>CRAN</p>"},{"location":"software/installing_software/","title":"Installing software","text":""},{"location":"software/installing_software/#checking-if-a-package-is-installed","title":"Checking if a package is installed","text":"<p>Note</p> <p>Before installing software yourself, check if it has already been installed in a system-wide module or by one of your research colleagues.</p> <p>Some common software packages are already available as part of katana's the operating system. They are listed by: </p> <pre><code>    yum list installed\n</code></pre> <p>Warning</p> <p>Do not try to run system commands as a user. These include: apt-get install, yum install, su, or sudo. </p> <p>As detailed in Environment Modules, software already installed by HPC staff is listed with:</p> <pre><code>   module avail\n</code></pre>"},{"location":"software/installing_software/#installing-a-binary-package","title":"Installing a binary package","text":"<p>While installing from source is preferred for effeciency reasons, sometimes only precompiled binaries. When downloading binaries, make sure you select the correct architecure and operating system for katana.</p> <p>Note</p> <p>Centos 7, 64-bit, Intel x86_6 / AMD64</p> <pre><code>wget https://website.org/binary/application  </code></pre> <p>Change file permissions to make the application readable and executable</p> <pre><code>chmod u+rx ./application\n</code></pre>"},{"location":"software/installing_software/#compiling-from-source","title":"Compiling from source","text":"<p>Compiling from source is preferred for efficiency reasons, but is a more involved process than binary installation. </p> <p>Note</p> <p>It is best to install your software in your home directory, since it is backed up every night. </p>"},{"location":"software/installing_software/#github-cloning","title":"Github cloning","text":"<p>Source code is commonly stored on GitHub for easy version control. Git is available by default on katana. Remember: UNSW has its own GitHub organisation.</p> <p>Copy the web address revealed by the green 'Code' button on the repository. Creating a local copy of the repository is then as simple as:</p> <pre><code>   git clone https://github.com/project/project.git\n</code></pre> <p>The created folder will contain the source code and some documentation files.</p>"},{"location":"software/installing_software/#readme-and-install-files","title":"README and INSTALL files","text":"<p>The README file contains general information for the software, and often a brief installation guide. INSTALL will contain more detailed installation instructions, including configuration for certain archictures. Please read the README and INSTALL files in full before attempting compilation.</p>"},{"location":"software/installing_software/#compilers","title":"Compilers","text":"<p>Try to use the system compilers <code>gcc</code> and <code>ld</code>. However, many codes require specific compilers and versions. Katana has many compilers available as modules including the Intel Compilers and Software Libraries</p> <p>Note</p> <p>Please install software using an interactive session, qsub -I, not directly on the login node. </p>"},{"location":"software/installing_software/#configuring-installation-files","title":"Configuring installation files","text":"<p>Commonly, a configuration script is available. This allows you to set where the software is installed by using the --prefix flag. Again, it is best to install software within your home directory </p> <pre><code>   ./configure --prefix=$HOME/apps/{PACKAGE}/{VERSION}  make &amp;&amp; make install\n</code></pre> <p>The software can then be installed according to the rules in the MakeFile. This is typically invoked with</p> <pre><code>   make &amp;&amp; make install\n</code></pre>"},{"location":"software/installing_software/#creating-module-files","title":"Creating module files","text":"<p>Much like katana's <code>Environment Modules</code>, you can also have multiple versions of the application available through your own modules.</p> <p>The template for environment modules is in:</p> <pre><code>/apps/modules/templates/module_file\n</code></pre> <p>The template module file makes some assumptions and examples, which may not be applicable to your software. Key sections will likely need to modify are:</p> <ul> <li>set      basepath          $env(HOME)/apps/{SOFTWARE_NAME}</li> <li> <p>set      version           {VERSION_NUMBER}        </p> </li> <li> <p>set      url</p> </li> <li> <p>set      installed</p> </li> <li> <p>set      compiled_with</p> </li> <li> <p>set      mpiversion</p> </li> <li> <p>prereq     {PREREQUISITE_SOFTWARE)</p> </li> </ul> <p>Note</p> <p>Insert \"module use --append $HOME/apps/Modules\" into your ~/.bashrc to enable using your own modules upon login. </p> <p>You should be able to module load your own software module as your own.</p>"},{"location":"software/installing_software/#r-and-python-packages","title":"R and Python Packages","text":"<p>Many R and Python packages are installed on katana. We have specific documentation to install your own packages in R and Python`</p> <p>Note</p> <p>If you do need assistance with software install after trying yourself, then send an email to IT Service Centre with Katana in the subject line.</p>"},{"location":"software/intel_compilers_and_libraries/","title":"Intel Compilers and Software Libraries","text":"<p>Research Technology Services has a licence for Intel Compiler Collection which can be accessed by loading a module and contains 3 groups of software, namely compilers, libraries and a debugger. This software has been optimised by Intel to take advantage of the specific capabilities of the different intel CPUs installed in the Intel based clusters.</p> <ul> <li>Compilers<ul> <li>Intel C Compiler (icc)</li> <li>Intel C++ Compiler (icpc)</li> <li>Intel Fortran Compiler (ifort)</li> </ul> </li> <li>Libraries<ul> <li>Intel Math Kernel Library (MKL)</li> <li>Intel Threading Building Blocks (TBB)</li> <li>Intel Integrated Performance Primitives (IPP)</li> </ul> </li> <li>Debugger<ul> <li>Intel Debugger (idbc)</li> </ul> </li> </ul>"},{"location":"software/java/","title":"Java","text":"<p>Java is installed as part of the Operating System but we would strongly recommend against using that version - we cannot guarantee scientific reproducibility with that version. Please use the java modules. </p> <p>Each Java module sets </p> <pre><code>    _JAVA_TOOL_OPTIONS -Xmx1g\n</code></pre> <p>This sets the heap memory to 1GB. If you need more, set the environment variable <code>_JAVA_OPTIONS</code> which overrides <code>_JAVA_TOOL_OPTIONS</code></p> <pre><code>    export _JAVA_OPTIONS=\"-Xmx5g\"\n</code></pre>"},{"location":"software/jupyter-notebooks/","title":"Jupyter Notebooks","text":"<p>Jupyter Notebooks and JupyterLab are best run in Katana OnDemand. </p> <p>Katana OnDemand comes with some built in environment and kernels that are  available for use out of the box.</p>"},{"location":"software/jupyter-notebooks/#python-virtual-environments","title":"Python virtual environments","text":"<p>If you need to use the Jupyter Notebook or Jupyter Lab with your own  Python Virtual Environments you will need to create your own Python Jupyter kernel. Here is an example:</p>"},{"location":"software/jupyter-notebooks/#create-and-load-the-virtual-environment","title":"Create and load the virtual environment","text":"<pre><code>$ module load python/3.8.3\n$ python3 -m venv --system-site-packages /home/z1234567/.venvs/jupyter-kernel\n$ source /home/z1234567/.venvs/jupyter-kernel/bin/activate\n</code></pre> <p>Create the Jupyter Kernel </p> Using the helper scriptInstalling the kernel manually <p>This script can automatically setup Jupyter kernels for use in Katana OnDemand.</p> <pre><code>(jupyter-kernel) $ install_jupyter_kernels\n</code></pre> <pre><code>(jupyter-kernel) $ python3 -m ipykernel install --prefix=$HOME/.local --name=jlab-kernel\n</code></pre> <p>Warning</p> <p>The <code>--name=XXXX</code> isn't strictly necessary, but if you don't use it,  the kernel will be called \"Python 3\". This will not be distinguishable from the Katana supplied Python 3 and could cause confusion.</p> <p>Now when you load a JupyterLab session, you should see your personal kernel  in the list of available kernels. This kernel will have access to your virtual environment.</p>"},{"location":"software/jupyter-notebooks/#conda-environments","title":"Conda environments","text":"<p>If you need to use the Jupyter Notebook or Jupyter Lab with your own Conda environment you will need to create your own Python or R Jupyter kernel. Here is an example:</p>"},{"location":"software/jupyter-notebooks/#create-and-activate-the-environment","title":"Create and activate the environment","text":"<pre><code>$ conda create -n my_conda_env -c conda-forge ipykernel\n$ conda activate my_conda_env\n</code></pre>"},{"location":"software/jupyter-notebooks/#create-the-jupyter-kernel","title":"Create the Jupyter Kernel","text":"<p>You can alternatively install the kernel manually: <code>Installing the kernel manually</code>_</p> <pre><code>(my_conda_env) $ install_jupyter_kernels\n</code></pre> <p>Kernels other than <code>ipykernel</code>, need <code>jupyter</code> to be installed in addition to the kernel.</p> <p>For example, to use the R kernel <code>r-irkernel</code>, you must also install the <code>jupyter</code> package before running the helper script.</p> <pre><code>(my_conda_env) $ conda install jupyter\n</code></pre>"},{"location":"software/matlab/","title":"Matlab","text":""},{"location":"software/matlab/#running-interactively","title":"Running interactively","text":"<p>Interactive sessions of matlab are best run on Katana OnDemand. </p>"},{"location":"software/matlab/#batch-jobs","title":"Batch Jobs","text":"<p>You can run matab within a batch job. The example below shows the flags used to start matlab without a graphical interface. The matlab script (scriptfile.m) needs to be in the same directory as qsub was run to submit the batch job. </p> <pre><code>module load matlab/R2022a\n\nmatlab -nodisplay -nosplash -r scriptfile\n</code></pre> <p>If you wish to submit the job from a different directory than your matlab script, you need to provide the full file path to cd (change directory) command with matlab. Note the use of quotes.</p> <pre><code>module load matlab/R2022a\n\nmatlab -nodisplay -nosplash -r \"cd('/path/to/script/');scriptfile\"\n</code></pre> <p>Later versions of matlab provide the '-batch' flag as an alternative. </p> <pre><code>module load matlab/R2022a\n\nmatlab -batch -r scriptfile\n</code></pre>"},{"location":"software/operating_systems/","title":"Operating Systems","text":"<p>Katana nodes are running either RedHat 7.8 (management plane) or CentOS 7.8 (compute nodes). </p> <p>Research software is installed in modules so that they can be loaded or unloaded as necessary. This way we can offer and run multiple versions of each package at the same time.</p>"},{"location":"software/perl/","title":"Perl","text":"<p>The default version of Perl on Katana is 5.16.3 which is provided by CentOS 7 and can be found at <code>/usr/bin/perl</code>.</p> <p>This is an older version of Perl. We have Perl 5.28.0 installed as a module. </p> <p>It is common for perl scripts to begin with </p> <pre><code>#!/usr/bin/perl\n</code></pre> <p>If you are using the Perl module, you will need to change the first line to </p> <pre><code>#!/usr/bin/env perl\n</code></pre>"},{"location":"software/python/","title":"Python","text":"<p>It is common for python scripts to begin with </p> <pre><code>#!/usr/bin/python\n</code></pre> <p>If you are using a Python module, you will need to change the first line to </p> <pre><code>#!/usr/bin/env python\n</code></pre> <p>or more likely</p> <pre><code>#!/usr/bin/env python3\n</code></pre>"},{"location":"software/python/#conda-and-anaconda","title":"Conda and Anaconda","text":"<p>We get a lot of questions about installing Conda and Anaconda. Unfortunately neither are designed to be installed in multi-user environments.</p> <p>You are able to install them into your home directory and we encourage you to do so.</p> <p>Alternatively, many packages will give you an option for a <code>pip install</code> - if this is an option, we recommend you use <code>python virtual environments</code>.</p> <p>Note</p> <p>If you are using Conda for GPU-enabled software, make sure it is installed on a GPU node during an interactive session.</p>"},{"location":"software/python/#python-virtual-environments","title":"Python Virtual Environments","text":"<p>We encourage researchers to utilise the power of Python Virtual Environments if they are developing their own software or want to use packages that aren't installed.</p>"},{"location":"software/python/#background","title":"Background","text":"<p>Sometimes you will need to use a particular version of Python, or you will need to use a set of Python libraries that aren't available in the provided installation.</p> <p>In these cases, we can use what's known as a virtual environment or venv. A venv gives us a static version of Python in our home directory in which we can install any packages we like. </p> <p>In this walk through we will see how to set up a venv and explain what's happening under the hood. Then we will show how you can use one in your development process. As a note, in this walk through we will refer to packages and libraries as \"packages\". When we use these terms, we are referring to a collection of files, written in Python, usually with some versions and potentially some requirements of their own. </p>"},{"location":"software/python/#setting-up-the-default-environment","title":"Setting up the default environment","text":"<p>According to the Python documentation, we will run something like this <code>python3 -m venv /path/to/new/virtual/environment</code>. This is not a directory that we need to see or need to spend time in actively, so it's ok to make it hidden. This is an important distinction - the venv should not be where you are doing your development. It's meant to be flexible - as soon as you fill it with development code, it's no longer flexible. Also, you want your development code backed up or in a repository - it is unnecessary bloat to add the Python software to that backup or repository.</p> <p>We will make a directory in which we can keep many venvs. What I found was that once I started using venvs, it didn't make any sense to do Python development without them.</p> <p>In Linux we can make a directory or file invisible by naming it with a leading dot:</p> <pre><code>[z1234567@katana2 ~]$ mkdir /home/z1234567/.venvs/\n</code></pre>"},{"location":"software/python/#setting-up-the-virtual-environment-creation-and-activation","title":"Setting up the virtual environment - creation and activation","text":"<p>I'll be using the latest version of Python available to me. Since this is one of the modules that we offer, I can use it with the understanding that it will be there indefinitely.</p> <pre><code>[z1234567@katana2 ~]$ module load python/3.7.4\n[z1234567@katana2 ~]$ which python3\n/apps/python/3.7.4/bin/python3\nz1234567@katana2 ~]$ python3 -m venv /home/z1234567/.venvs/venv-tutorial-1\n</code></pre> <p>That's it, we are done. If you want to take a look under the hood, see Virtual Environments from the inside</p> <p>Note</p> <p>We use the command <code>which</code> to show the path of the executable. It's an example command that you do not need to replicate unless you are checking your work. </p> <p>Next, we need to activate our venv. This makes our virtualenv our current environment. To activate, we execute <code>source /path/to/venv/bin/activate</code>. Note that after activation, the prompt changes to make it clear you are now in a venv. You can see the change in which versions of <code>python3</code> and <code>pip3</code> are available before and after activation:</p> <p>Before we activate our environment</p> <pre><code>[z1234567@katana2 ~]$ which python3; which pip3\n/apps/python/3.7.4/bin/python3\n/apps/python/3.7.4/bin/pip3\n</code></pre> <p>Activation</p> <pre><code>[z1234567@katana2 ~]$ source ~/.venvs/venv-tutorial-1/bin/activate\n</code></pre> <p>After activation, our python binaries are not the defaults, but the versions in our venv</p> <pre><code>(venv-tutorial-1) [z1234567@katana2 ~]$ which python3; which pip3\n~/.venvs/venv-tutorial-1/bin/python3\n~/.venvs/venv-tutorial-1/bin/pip3\n</code></pre>"},{"location":"software/python/#pip3-the-python-package-manager-the-package-installer-for-python","title":"pip3 - the Python package manager (\"the Package Installer for Python\")","text":"<p>Using pip3 we can see whats installed and install new packages. You will often see packages give installation advice for pip (Conda is another popular system).</p> <p>Now that we are using the venv, we can list what's in the venv, and then install a new package:</p> <pre><code>(venv-tutorial-1) [z1234567@katana2 ~]$ pip3 list\nPackage    Version\n---------- -------\npip        19.0.3 \nsetuptools 40.8.0 \nYou are using pip version 19.0.3, however version 20.0.2 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n</code></pre> <p>At this point - before any work is done, and while using your venv - it's a great time to perform that update.</p> <pre><code>(venv-tutorial-1) [z1234567@katana2 ~]$ pip install --upgrade pip\nCollecting pip\n    Downloading https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl (1.4MB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.4MB 1.5MB/s \nInstalling collected packages: pip\n    Found existing installation: pip 19.0.3\n    Uninstalling pip\\-19.0.3:\n    Successfully uninstalled pip\\-19.0.3\nSuccessfully installed pip-20.0.2\n(venv-tutorial-1) [z1234567@katana2 ~]$ pip install --upgrade setuptools\nCollecting setuptools\n    Downloading setuptools-46.1.1-py3-none-any.whl (582 kB)\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 582 kB 13.5 MB/s \nInstalling collected packages: setuptools\n    Attempting uninstall: setuptools\n    Found existing installation: setuptools 40.8.0\n        Uninstalling setuptools\\-40.8.0:\n            Successfully uninstalled setuptools\\-40.8.0\nSuccessfully installed setuptools-46.1.1\n(venv-tutorial-1) [z1234567@katana2 w~]$ pip3 list\nPackage    Version\n---------- -------\npip        20.0.2 \nsetuptools 46.1.1 </code></pre>"},{"location":"software/python/#installing-software","title":"Installing software","text":"<p>And then package installation is as easy as using <code>pip install ...</code>:</p> <pre><code>(venv-tutorial-1) [z1234567@katana2 ~]$ pip install numpy\nCollecting numpy\n    Downloading numpy-1.18.2-cp37-cp37m-manylinux1*x86*64.whl (20.2 MB)\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.2 MB 38 kB/s \nInstalling collected packages: numpy\nSuccessfully installed numpy-1.18.2\n(venv-tutorial-1) [z1234567@katana2 ~]$ pip list\nPackage    Version\n---------- -------\nnumpy      1.18.2 \npip        20.0.2 \nsetuptools 46.1.1 </code></pre>"},{"location":"software/python/#exiting-the-venv-and-coming-around-again","title":"Exiting the venv, and coming around again","text":"<p>To leave a venv, you use the <code>deactivate</code> command like this:</p> <pre><code>(venv-tutorial-1) [z1234567@katana2 ~]$ deactivate [z1234567@katana2 ~]$\n</code></pre> <p>Notice how the prompt returned to the way it was? Let's create a new venv:</p> <pre><code>[z1234567@katana2 ~]$ python3 -m venv /home/z1234567/.venvs/scipy-example\n[z1234567@katana2 ~]$ ls -l ~/.venvs/\ntotal 0\ndrwx------. 5 z1234567 unsw 69 Mar 23 15:07 scipy-example\ndrwx------. 5 z1234567 unsw 69 Mar 23 11:45 venv-tutorial-1\n[z1234567@katana2 ~]$ source ~/.venvs/scipy-example/bin/activate\n(scipy-example) [z1234567@katana2 src]$ (scipy-example) [z1234567@katana2 src]$ pip list\nPackage    Version\n---------- -------\npip        19.0.3 \nsetuptools 40.8.0 \nYou are using pip version 19.0.3, however version 20.0.2 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n</code></pre> <p>When we install SciPy, it automatically knows to install NumPy, a dependency:</p> <pre><code>(scipy-example) [z1234567@katana2 ~]$ pip install scipy\nCollecting scipy\n    Downloading scipy-1.4.1-cp37-cp37m-manylinux1*x86*64.whl (26.1 MB)\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.1 MB 95 kB/s \nCollecting numpy&gt;=1.13.3\n    Using cached numpy-1.18.2-cp37-cp37m-manylinux1*x86*64.whl (20.2 MB)\nInstalling collected packages: numpy, scipy\nSuccessfully installed numpy-1.18.2 scipy-1.4.1\n(scipy-example) [z1234567@katana2 ~]$ pip list\nPackage    Version\n---------- -------\nnumpy      1.18.2 \npip        20.0.2 \nscipy      1.4.1  \nsetuptools 46.1.1 </code></pre> <p>If you want to install an older version, it's relatively easy</p> <pre><code>(old-scipy-example) [z1234567@katana2 ~]$ pip install scipy==1.2.3\nCollecting scipy==1.2.3\n    Downloading https://files.pythonhosted.org/packages/96/e7/e06976ab209ef44f0b3dc638b686338f68b8a2158a1b2c9036ac8677158a/scipy-1.2.3-cp37-cp37m-manylinux1_x86_64.whl (24.8MB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24.8MB 239kB/s \nCollecting numpy&gt;=1.8.2 (from scipy==1.2.3)\nUsing cached https://files.pythonhosted.org/packages/b7/ce/d0b92f0283faa4da76ea82587ff9da70104e81f59ba14f76c87e4196254e/numpy-1.18.2-cp37-cp37m-manylinux1_x86_64.whl\nInstalling collected packages: numpy, scipy\nSuccessfully installed numpy-1.18.2 scipy-1.2.3\n(old-scipy-example) [z1234567@katana2 src]$ pip list\nPackage    Version\n---------- -------\nnumpy      1.18.2 \npip        20.0.2 \nscipy      1.2.3  \nsetuptools 46.1.1 </code></pre> <p>That's a quick introduction to how you can install Python packages locally. </p>"},{"location":"software/python/#special-cases","title":"Special Cases","text":"<p>Say for instance you want to use software X in a Jupyter Notebook. X is already installed on Katana.</p> <p>In that case, your workflow would be:</p> <ul> <li>load the module in question</li> <li>create the Virtual Environment with the flag <code>--system-site-packages</code></li> <li>install software in question with an understanding that you might not be able to get the latest release</li> </ul> <p>For example, using the Katana TensorFlow installation and a desire for Jupyter:</p> <pre><code>[z1234567@katana1 ~]$ module load tensorflow/1.14gpu\n[z1234567@katana1 ~]$ python3 -m venv /home/z1234567/.venvs/tf --system-site-packages\n[z1234567@katana1 ~]$ source ~/.venvs/tf/bin/activate\n(tf) [z1234567@katana2 ~]$ pip install jupyter\n</code></pre> <p>This will throw errors because there are a collection of packages missing in relation to the latest Jupyter. They shouldn't affect your ability to run Jupyter Notebooks with tensorflow.</p>"},{"location":"software/python/#virtual-environments-from-the-inside","title":"Virtual Environments from the inside","text":"<p>We've built a venv in our <code>~/.venvs</code> directory. Let's take a look inside. This presumes you have used the command </p> <pre><code>[z1234567@katana2 src]$ python3 -m venv /home/z1234567/.venvs/venv-tutorial-1\n</code></pre> <p>to set up your virtualenv.</p> <p>Here is a quick overview of the basics.</p> <p>We can see there is a directory in <code>~/.venvs</code> that has the same name as the virtualenv we created.</p> <pre><code>[z1234567@katana2 ~]$ ls -l ~/.venvs/\ntotal 0 \ndrwx------.  5 z1234567 unsw   69 Mar 23 11:45 venv-tutorial-1\n</code></pre> <p>Inside that directory we can see some more directories. The two important directories here are <code>bin</code> and <code>lib</code>.</p> <pre><code>[z1234567@katana2 ~]$ ls -l ~/.venvs/venv-tutorial-1/\ntotal 8\ndrwx------. 2 z1234567 unsw 4096 Mar 23 11:45 bin\ndrwx------. 2 z1234567 unsw    6 Mar 23 11:45 include\ndrwx------. 3 z1234567 unsw   22 Mar 23 11:45 lib\nlrwxrwxrwx. 1 z1234567 unsw    3 Mar 23 11:45 lib64 -&gt; lib\n-rw-------. 1 z1234567 unsw   83 Mar 23 11:45 pyvenv.cfg\n</code></pre> <p>In <code>bin</code> you will see executables. The main one of note is <code>activate</code>.</p> <pre><code>[z1234567@katana2 ~]$ ls -l ~/.venvs/venv-tutorial-1/bin/\ntotal 36\ndrwx------. 2 z1234567 unsw 4096 Mar 23 11:45 .\ndrwx------. 5 z1234567 unsw   69 Mar 23 11:45 ..\n-rw-r--r--. 1 z1234567 unsw 2235 Mar 23 11:45 activate\n-rw-r--r--. 1 z1234567 unsw 1291 Mar 23 11:45 activate.csh\n-rw-r--r--. 1 z1234567 unsw 2443 Mar 23 11:45 activate.fish\n-rwxr-xr-x. 1 z1234567 unsw  266 Mar 23 11:45 easy_install\n-rwxr-xr-x. 1 z1234567 unsw  266 Mar 23 11:45 easy_install-3.7\n-rwxr-xr-x. 1 z1234567 unsw  248 Mar 23 11:45 pip\n-rwxr-xr-x. 1 z1234567 unsw  248 Mar 23 11:45 pip3\n-rwxr-xr-x. 1 z1234567 unsw  248 Mar 23 11:45 pip3.7\nlrwxrwxrwx. 1 z1234567 unsw    7 Mar 23 11:45 python -&gt; python3\nlrwxrwxrwx. 1 z1234567 unsw   30 Mar 23 11:45 python3 -&gt; /apps/python/3.7.4/bin/python3\n</code></pre> <p>In <code>lib</code> we need to traverse a few more directories, but eventually we will see where the packages are installed. As you can see, <code>pip</code> and <code>setuptools</code> are already installed. These are the default:</p> <pre><code>[z1234567@katana2 ~]$ ls -l ~/.venvs/venv-tutorial-1/lib/python3.7/site-packages/\ntotal 16\n-rw-------. 1 z1234567 unsw  126 Mar 23 11:45 easy_install.py\ndrwx------. 5 z1234567 unsw   90 Mar 23 11:45 pip\ndrwx------. 2 z1234567 unsw 4096 Mar 23 11:45 pip-19.0.3.dist-info\ndrwx------. 5 z1234567 unsw   89 Mar 23 11:45 pkg_resources\ndrwx------. 2 z1234567 unsw   40 Mar 23 11:45 **pycache**\ndrwx------. 6 z1234567 unsw 4096 Mar 23 11:45 setuptools\ndrwx------. 2 z1234567 unsw 4096 Mar 23 11:45 setuptools-40.8.0.dist-info\n</code></pre>"},{"location":"software/r/","title":"R and RStudio","text":"<p>R is installed as a module. Each version has a number of libraries installed  within it.</p> <p>If you would like a new library installed, please email the  IT Service Centre with \"Katana R Module  installation\" in the subject line.</p> <p>If you would like to use RStudio, we recommend you use the Katana OnDemand service.</p>"},{"location":"software/r/#installing-libraries","title":"Installing libraries","text":"<p>Please try installing R libraries yourself before contacting the service desk. </p> <p>If you want to install a library from CRAN_, github or your own R library for  testing or non-general usage, you can use the regular method and the library  will be installed locally:</p> <p>Create directory inside your home directory for the source code, and clone the library (if using Git):</p> <pre><code>mkdir ~/src\ncd ~/src\ngit clone https://github.com/user/mypackage </code></pre> <p>Start R or RStudio and install </p> <pre><code>    &gt; library('devtools')\n&gt; getwd()\n[1] \"/home/z1234567/src\"\n&gt; install('mypackage')\n...\n* DONE (mypackage)\n</code></pre> <p>CRAN</p>"},{"location":"software/sas/","title":"SAS","text":"<p>The 64-bit version of SAS is available as a module.</p> <p>By default SAS will store temporary files in <code>/tmp</code> which can easily fill up, leaving the node offline. In order to avoid this we have set the default to <code>$TMPDIR</code> to save temporary files in <code>/var/tmp</code> on the Katana head node and local scratch on compute nodes. If you wish to save temporary files to a different location you can do that by using the <code>-work</code> flag with your SAS command or adding this line to your <code>sasv9.cfg</code> file:</p> <pre><code>    -work /my/directory\n</code></pre>"},{"location":"software/stata/","title":"Stata","text":"<p>Stata is available as a module. </p> <p>When using Stata in a pbs batch script, the syntax is</p> <pre><code>stata -b do StataClusterWorkshop.do\n</code></pre> <p>If you wish to load or install additional Stata modules or commands you should use findit command on your local computer to find the command that you are looking for. Then create a directory called <code>myadofiles</code> in your home directory and copy the .ado (and possibly the .hlp) file into that directory. Now that the command is there it just remains to tell Stata to look in that directory which can be done by using the following Stata command.</p> <pre><code>sysdir set PERSONAL $HOME/myadofiles\n</code></pre>"},{"location":"software/tmux/","title":"TMUX","text":"<p>tmux_ is available on Katana. It can be used to manage multiple sessions, including keeping them alive despite a terminal losing connectivity or being shutdown.</p> <p>When you login to Katana using the terminal, it is a \"live\" session - if you close the terminal, the session will also close. If you shut your laptop or turn off the network, you will also kill the session.</p> <p>This is fine, except when you have a long running program - say you are downloading a large data set - and you need to leave campus.</p> <p>In these situations, you can use <code>tmux</code> to create an interruptible session. <code>tmux</code> has other powerful features - multiple sessions and split screens being two of many features.</p> <p>To start tmux, type <code>tmux</code> at the terminal. A new session will start and there will be a green information band at the bottom of the screen. </p> <p>Anything you start in this session will keep running even if you are disconnected from that session regardless of the reason. Except when the server itself is rebooted. That - and I hope this is obvious - will kill all sessions.</p> <p>If you do get detached, you can re-attach by logging into the same server and using the command <code>tmux a</code></p> <p>tmux is similar to another program called <code>screen</code> which is also available. </p> <p>tmux</p>"},{"location":"software/zip/","title":"Zip","text":""},{"location":"software/zip/#compressing-large-directories","title":"Compressing Large Directories","text":"<p>If you want to compress large directories or directories with a large number of files, we recommend a simple tool called tgzme_ developed by one of our researchers.</p> <p>It's essentially a smart shell script wrapped around the one line command:</p> <pre><code>tar -c $DIRECTORY | pigz &gt; $DIRECTORY.tar.gz\n</code></pre> <p>We thank <code>Dr. Edwards</code> for his contribution.</p> <p>tgzme</p> <p>Dr. Edwards</p>"},{"location":"storage/cloudstor/","title":"CloudStor","text":"<p>You can access CloudStor from Katana Data Mover. RClone is installed on KDM so you can upload data into CloudStor. </p>"},{"location":"storage/cloudstor/#configure-rclone","title":"Configure RClone","text":"<ol> <li>First, you will need to <code>set your Sync Password</code> in the  <code>CloudStor settings page</code>, and keep a copy locally.</li> </ol> <p>Warning</p> <p>This can be confusing. You need to think of the CloudStor web interface and the data store as two different logins. The Sync  password is for data transfer only and is different from the web login password. </p> <ol> <li>Next, login to KDM and run:</li> </ol> <pre><code>[z1234567@kdm ~]$ rclone config\n</code></pre> <p>You will be asked a set of questions. The short answers are:</p> <ol> <li>new remote ('n')</li> <li>name ('CloudStor')</li> <li>storage type webdav ('25' in current installation)</li> <li>url for http connection (https://cloudstor.aarnet.edu.au/plus/remote.php/webdav/)</li> <li>Name of webdav site: OwnCloud ('2' in current installation)</li> <li>user name per your cloudstor settings page (first.last@unsw.edu.au, not zID@unsw.edu.au)</li> <li>Password ('Yes')</li> <li>enter password (paste or type in the Sync password from step 1 above)</li> <li>confirm password (paste of type in the Sync password from step 1 above)</li> <li>bearer_token: (blank)</li> </ol> <p>You should then see something like this to which you should answer yes:</p> <pre><code>    Remote config\n    --------------------\n    [CloudStor]\ntype = webdav\n    url = https://cloudstor.aarnet.edu.au/plus/remote.php/webdav/\n    vendor = owncloud\n    user = first.last@unsw.edu.au\n    pass = *** ENCRYPTED ***\n    --------------------\n    y) Yes this is OK\n    e) Edit this remote\n    d) Delete this remote\n    y/e/d&gt; y\n</code></pre> <p>Then quit and test:</p> <pre><code>[z1234567@kdm ~]$ rclone copy --progress --transfers 8 testdata CloudStor:/\n</code></pre> <p>If it doesn't work, you can edit the config or make a new one.</p> <p>The 'name' field you are asked at step two of the config is what the config and  end point will be known as. If you put in the name <code>Murgatroyd</code> then you  would have to access the instance like this:</p> <pre><code>[z1234567@kdm ~]$ rclone copy --progress --transfers 8 testdata Murgatroyd:/\n</code></pre> <p>CloudStor</p> <p>CloudStor settings page</p> <p>set your Sync Password</p>"},{"location":"storage/data_archive/","title":"How to use the UNSW Data Archive","text":"<p>The UNSW Data Archive is the primary research storage facility provided by UNSW. The Data Archive gives UNSW researchers a free, safe and secure storage service to store and access research data well beyond the life of the project that collected that data.</p> <p>To help researchers make use of this system the Katana Data Mover has a script that you can use to copy files from Katana into a project on the Data Archive system.</p> <p>Note</p> <p>To use this script you must have access to the UNSW Data Archive which requires setting up a Research Data Management Plan.</p> <p>The best documentation on how to use the <code>Data Archive</code> is found on their website:</p> <ul> <li>using the <code>web application</code></li> <li>using <code>SFTP</code></li> <li>using the <code>Command Line</code></li> </ul> <p>To see what versions of the Data Archive script are available log on to <code>kdm.restech.unsw.edu.au</code> and type</p> <pre><code>module avail unswdataarchive\n</code></pre> <p>Use the help command for usage</p> <pre><code>module help unswdataarchive/2020-03-19\n</code></pre>"},{"location":"storage/data_archive/#initial-setup","title":"Initial Setup","text":"<p>To use the Data Archive you need to set up a configuration file. Here's how to create the generic config in the directory you are in:</p> <pre><code>[z1234567@kdm ~]$ module add unswdataarchive/2020-03-19\n[z1234567@kdm ~]$ get-config-file\n</code></pre> <p>To generate a token send an email to the IT Service Centre asking for a Data Archive token to be generated. A service desk request for an authentication token to be generated needs to indicate a Data Archive namespace (/UNSW_RDS/Dxxx or /UNSW_RDS/Hxxx) as a scope for the token. Your Data Archive namespace is recorded in the Data Archive welcome email.</p> <p>Warning</p> <p>This advice has poor results. The help file is too long for most screen sizes and there's no pagination in modules version &lt; 4. Last line should include a location that the researcher can read directly (using less)</p> <p>Then edit the configuration file <code>config.cfg</code> and change the line that looks like <code>token=</code></p> <p>If you haven't generated a token you can also upload content using your zID and zPass by adding the following line to the file <code>config.cfg</code> and you will be asked for your zPass when you start the upload.</p> <pre><code>user=z1234567\n</code></pre>"},{"location":"storage/data_archive/#starting-a-data-transfer","title":"Starting a data transfer","text":"<p>To get data into the archive, we use <code>upload.sh</code></p> <pre><code>upload.sh /path/to/your/local/directory /UNSW_RDS/D0000000/your/collection/name\n</code></pre> <p>To get data from the archive, we use <code>download.sh</code></p> <pre><code>download.sh /UNSW_RDS/D0000000/your/collection/name /path/to/your/local/directory\n</code></pre> <p>Data Archive</p> <p>web application</p> <p>SFTP</p> <p>Command Line</p>"},{"location":"storage/kdm/","title":"Katana Data Mover","text":"<p>Also known as <code>kdm</code> or <code>kdm.restech.unsw.edu.au</code></p> <p>If you have data that you would like to copy to or within the Katana cluster, archive or even compress and decompress you should use the Katana Data Mover - also known as the KDM server - rather than using the head node. This section contains instructions on how to use KDM server.</p> <p>If you are familiar with using Linux commands to copy or move files then you can do that directly by logging on to <code>kdm.restech.unsw.edu.au</code> via <code>ssh</code> in the same way that you would log in to Katana and then use the <code>cp</code>, <code>mv</code> and <code>rsync</code> commands that you would normally use under Linux.</p> <p>If you are not familiar with using the Linux command line for moving or copying files then the easiest way to move files around is to use client software such as FileZilla_. Once you have connected to <code>kdm.restech.unsw.edu.au</code> using your zID and zPass you should see a remote view which corresponds to the files sitting on Katana. You can then use the FileZilla interface to move files and folders around.</p> <p>Note</p> <p>We require people to \"move data\" through the data mover. We have hundreds of users, most of whom have data ranging from very large to impossibly large. This is why we have the <code>kdm</code>. If you are transferring a couple of small text files - job scripts for instance - you can copy directly to the Katana. But we would ask you to keep it to a minimum, and nothing bigger than 2-3 MB.</p>"},{"location":"storage/kdm/#copying-files-to-and-from-a-cluster","title":"Copying Files To and From a Cluster","text":"<p>The method of transferring files to and from clusters depends on your local machine. If you are a Linux user then you should use rsync and if you are a Windows user then you should download and install WinSCP_ or FileZilla_</p> <p>Warning</p> <p>Filezilla is often marked as being a security risk. There is nothing that Research Technology Services can do about that - it's an issue for UNSW IT, Symantec, and FileZilla. If this happens to you, please try WinSCP_ - it's very good.</p>"},{"location":"storage/kdm/#filezilla","title":"Filezilla","text":"<p>Once you have installed Filezilla you can go into the site manager and create a new site in the site manager using the settings below.</p> <p> </p> filezilla site manager <p>You can also use the Quick Connect bar as shown here: </p> <p> </p> filezilla quick connect"},{"location":"storage/kdm/#from-my-computer-to-katana-home","title":"From my computer to Katana Home","text":"<p>To copy the directory <code>/home/1234567/my-directory</code> from your local computer to Katana scratch. The trailing <code>:</code> is important!</p> <pre><code>me@localhost:~$ rsync -avh /path/to/my-directory z1234567@kdm.restech.unsw.edu.au:\n</code></pre>"},{"location":"storage/kdm/#from-my-computer-to-katana-scratch","title":"From my computer to Katana Scratch","text":"<pre><code>me@localhost:~$ rsync -avh /path/to/my-directory z1234567@kdm.restech.unsw.edu.au:/srv/scratch/z1234567\n</code></pre>"},{"location":"storage/kdm/#from-katana-to-my-computer","title":"From Katana to my computer","text":"<p>First, you need to make sure the data is in either your Home directory or your scratch </p> <p>If the data is in <code>/home/z1234567/my-remote-results</code> and you want it in your home directory:</p> <pre><code>me@localhost:~$ rsync -avh z1234567@kdm.restech.unsw.edu.au:my-remote-results /home/me/\n</code></pre> <p>If the data is in <code>/srv/scratch/my-remote-results</code> and you want it in your home directory:</p> <pre><code>me@localhost:~$ rsync -avh z1234567@kdm.restech.unsw.edu.au:/srv/scratch/my-remote-results /home/me\n</code></pre> <p>Note</p> <p>TMUX is available if your data is large and the rsync might take a long time.</p>"},{"location":"storage/kdm/#mounting-university-provided-space-on-kdm","title":"Mounting University Provided Space on KDM","text":"<p>The university provides a large amount of space if you need. You can find more information about <code>staff storage</code> on the <code>UNSW website</code>.</p> <p>Filezilla</p> <p>WinSCP</p> <p>staff storage</p> <p>UNSW website</p>"},{"location":"storage/onedrive/","title":"OneDrive","text":"<p>You can mount Microsoft's OneDrive on Katana.</p>"},{"location":"storage/onedrive/#onedrive-background","title":"OneDrive Background","text":"<p>We don't recommend researchers mount their OneDrive because the configuration process is not ideal. However, it is possible with the following method.</p> <p><code>There are limitations to using RClone with OneDrive</code> that users will need to be  aware of and which the UNSW team cannot help with.</p> <p>Note: Mounting OneDrive locally will only work on the machine on which the <code>mount</code> command is run.</p> <p>Note</p> <p>Please mount OneDrive on Katana Data Mover, not the login nodes. </p> <p>The configuration section will probably only need to run once, whereas the \"how to mount\" section will need to be run each time.</p>"},{"location":"storage/onedrive/#prerequisites","title":"Prerequisites","text":"<ol> <li>OneDrive needs your consent to give rclone access to your files. </li> <li>We can't guarantee or test if this works for researchers in Germany or  China. OneDrive has a different set up for Germany and China due to local laws  about data storage.</li> <li>You will need at least one empty directory for mounting. Restech recommends creating the directory <code>/home/z1234567/OneDrive</code> to mount OneDrive onto.</li> </ol>"},{"location":"storage/onedrive/#configure-rclone-for-onedrive","title":"Configure RClone for OneDrive","text":"<ol> <li>Login to KDM and run:</li> </ol> <pre><code>[z1234567@kdm ~]$ rclone config\n</code></pre> <p>You will be asked a set of questions. The short answers are:</p> <ol> <li>new remote ('n')</li> <li>name ('OneDrive')</li> <li>storage type Microsoft OneDrive ('26' in rclone version 1.55.1)</li> <li>OAuth Client Id (Press Enter for the default)  </li> <li>OAuth Client Secret (Press Enter for the default)</li> <li>Choose national cloud region for OneDrive (\"1. global\")</li> <li>Edit advanced config? (\"n\")</li> <li>Remote config? (\"n\")</li> <li>On a machine with rclone and a web browser (not kdm): Run the <code>rclone authorize \"onedrive\" ...</code> command and copy to the clipboard. </li> <li>Back to kdm: Paste result: </li> <li>Choose a number from below, or type in an existing value (\"1. OneDrive Personal or Business\")</li> <li>Chose drive to use: (\"0\")</li> <li>Is that okay? (\"Y\")</li> </ol> <p>You should then see something like this to which you should answer yes:</p> <pre><code>   --------------------\n    [MS OneDrive]\ntype = onedrive\n    client_id = c8800f43-7805-46c2-b8b2-1c55f3859a4c\n    client_secret = SECRET\n    region = global\n    token = {\"access_token\":\"eyJ0e...asdasd\"}\ndrive_type = business\n    --------------------\n    y) Yes this is OK (default)\ne) Edit this remote\n    d) Delete this remote\n    y/e/d&gt; </code></pre>"},{"location":"storage/onedrive/#how-to-mount-onedrive","title":"How to mount OneDrive","text":"<p>Once logged in:</p> <ol> <li>Mount the drive. The basic syntax is:</li> </ol> <pre><code>rclone mount &lt;remote-name&gt;: /path/to/local/mount\n</code></pre> <p>We need to add a couple of flags to make this warning free and usable. Most  notably <code>--daemon</code> and <code>--vfs-cache-mode writes</code>.</p> <p>If you have followed the ResTech recommendations, your command will look like:</p> <pre><code>[z1234567@kdm ~]$ rclone mount OneDrive: /home/z1234567/OneDrive --daemon --vfs-cache-mode writes\n</code></pre> <p>Info</p> <p>Your OneDrive file contents should now be available at /home/z1234567/OneDrive (or chosen mount point). </p> <p>OneDrive</p> <p>OneDrive needs your consent</p> <p>There are limitations to using RClone with OneDrive</p>"},{"location":"storage/storage_locations/","title":"Storage Locations","text":"<p>The storage on Katana is split into several different types, each of which serves a different purpose. </p> <p>Important</p> <p>We have just said each of which serves a different purpose. Despite that, there will be overlap. And personal preference. In most cases it will be obvious where to put your information. If it isn't and you need help with your decision making, you can email the Research Data team team at rdm@unsw.edu.au for advice. They are friendly people.</p> <p>If you specifically wish to increase Katana storage allocations, please email the IT Service Centre - note that such increases are not automatic.</p>"},{"location":"storage/storage_locations/#storage-summary","title":"Storage  Summary","text":"Storage type Location Alias Purpose Backed up? Size limit Who has access Home drive /home/z1234567 $HOME Source code and programs Y 10Gb Only the user User scratch /srv/scratch/z1234567 /srv/scratch/$USER Data files N 128 GB Only the user Shared scratch /srv/scratch/group_name /srv/scratch/group_name Data files and programs shared by a team N Upon group requirements All users in the group Local scratch Intialised upon job running $TMPDIR Faster job completion with large datasets and temp files N 200Gb shared between node users Temporary for each job UNSW Research Storage /home/z1234567/sharename $HOME/sharename Storage of shared user and data files Y According to Data Management Plan"},{"location":"using_katana/about_katana/","title":"About Katana","text":"<p>Katana is a shared computational cluster located on campus at UNSW that has been designed to provide easy access to computational resources for groups working with non-sensitive data. It contains over 6,000 CPU cores, 8 GPU compute nodes (V100 and A100), and 6Pb of disk storage. Katana provides a flexible compute environment where users can run jobs that wouldn't be possible or practical on their desktop or laptop. For full details of the compute nodes including a full list see the compute node information section below.</p> <p>Katana is powerful on its own, but can be seen as a training or development base before migrating up to systems like Australia's peak HPC system Gadi_, located at NCI_. Research Technology Services also provide training, advice and support for new users or those uncertain if High Performance Computing is the right fit for their research needs.</p>"},{"location":"using_katana/about_katana/#system-configuration","title":"System Configuration","text":"<ul> <li>RPM based Linux OSes. RedHat on the management plane, CentOS on the nodes</li> <li>PBSPro_ version 19.1.3</li> <li>Large global scratch at <code>/srv/scratch</code>, local scratch at <code>$TMPDIR</code></li> <li>12, 48, 100, 200 hour Walltime queues with prioritisation</li> </ul>"},{"location":"using_katana/about_katana/#compute","title":"Compute","text":"<ul> <li>Heterogenous hardware: Dell, Lenovo, Huawei.</li> <li>Roughly 170 nodes</li> </ul>"},{"location":"using_katana/about_katana/#gpu-compute","title":"GPU Compute","text":"<p>The most popular use of these nodes is for Tensorflow.</p> <ul> <li>Eight GPU capable nodes<ul> <li>Tesla V100-SXM2, 32GB</li> <li>Nvidia A100, 40GB</li> </ul> </li> <li>Five are dedicated for the department that owns them</li> <li>Three are general use for all researchers</li> </ul> <p>You cannot use Tensorflow on the login nodes because they don't have GPUs. You will need to get access to a GPU node to do this. </p> <p>Warning</p> <p>Unfortunately, GPU nodes are in incredibly high demand.  We cannot provide special accommodation for any project.  You will need to wait in the common queue - or buy a GPU node for your group on which you will get priority</p> <p>To access a GPU node interactively, you can use a command like</p> <pre><code>[z1234567@katana ~]$ qsub -I -l select=1:ncpus=8:ngpus=1:mem=46gb,walltime=2:00:004\n</code></pre> <p>Note the 2 hour limit - that is the fastest way to get onto the GPU nodes. Unfortunately, there's no way to tell you that  your session has started, so you will need to monitor your command.</p> <p>.. We know that this isn't ideal and we wish there was an easier solution - we love making your lives easier. It's literally our jobs. But in this case, we don't have the resources available to make this faster, smoother or easier.  </p> <p>Info</p> <p>GPU-enabled software should be installed on a GPU node within an interactive session, in case the software is probing for GPU hardware or libraries.</p>"},{"location":"using_katana/accessing_katana/","title":"Accessing Katana","text":"<p>Anyone at UNSW can apply for a general account on Katana. This level is designed for those that think Katana would suit their research needs or will typically use less than 10,000 CPU hours a quarter. This level still gets access to the same level of support including software installation, help getting started or running their jobs. The only difference is the number of compute jobs that can be run at any time and how long they can run for - general users can only use a 12 hour Walltime.</p> <p>If your needs require more CPU hours or consulation, some Faculties, Schools and Research Groups have invested in Katana and have a higher level of access. Users in this situation should speak to their supervisor.</p>"},{"location":"using_katana/accessing_katana/#requesting-an-account","title":"Requesting an Account","text":"<p>To apply for an account you can send an email to the UNSW IT Service Centre giving your zID, your role within UNSW and the name of your supervisor or head of your research group.</p>"},{"location":"using_katana/accessing_katana/#connecting-to-katana","title":"Connecting to Katana","text":"<p>Info</p> <p>When you are connecting to Katana via <code>katana.restech.unsw.edu.au</code> you are connecting to one of two login nodes <code>katana1.restech.unsw.edu.au</code> or <code>katana2.restech.unsw.edu.au</code>. If you have a long running Tmux open, you will need to login to the node on which it was started.</p> <p>Platform</p> Linux and MacWindowsWindows subsystem for Linux (WSL) <p>From a Linux or Mac OS machine you can connect via ssh in a terminal:</p> <pre><code>laptop:~$ ssh z1234567@katana.restech.unsw.edu.au\n</code></pre> <p>From a Windows machine an SSH client such as PuTTY or MobaXTerm is required. </p> <p>If you are comfortable using PowerShell, OpenSSH is available on recent Windows versions. If not present, it can be installed on Windows 10. </p> <p>You can run a Linux environmet directly on Windows using Windows Subsystem for Linux (WSL).</p> <p>There are two ways to install WSL on your system:</p> <ol> <li>On UNSW Windows standard operating environment (SOE) machines you can open the Company Portal App and from there install one of the Linux distrubtions through the 'Apps', the same as you would other applications.</li> <li>Manually enable WSL in PowerShell and then install a Linux distribution through the Microsoft Store. </li> </ol> <p>Using WSL will not only let you connect to katana with SSH, but also provides many GNU/Linux tools that are useful when working with HPC and research data.</p>"},{"location":"using_katana/accessing_katana/#ssh-keepalive","title":"SSH KeepAlive","text":"<p>To stop your connection disconnecting after some idle time, you can send null packets to keep your session alive. You want to change the frequency of these packets from 0 (none) to a small time interval, say 60 seconds. The configuration differs depending on the SSH client used.</p> <p>On PuTTy: Category -&gt; Connection -&gt; \"Seconds between keepalives\"</p> <p>On MobaXterm: Settings -&gt; Configuration -&gt; SSH -&gt; SSH keepalive </p> <p>On Linux and WSL you send keepalive packets for all servers by editing ~/.ssh/config and adding the lines </p> <pre><code>   Host *\n      ServerAliveInterval 60\n</code></pre>"},{"location":"using_katana/accessing_katana/#graphical-sessions","title":"Graphical sessions","text":"<p>Warning</p> <p>Please use Katana OnDemand (kod.restech.unsw.edu.au) if available for your application. It is significantly easier to use for newcomers. </p> <p>Some software - Ansys, Jupyter Notebooks, Matlab, R and RStudio being among the most popular - are easier with a graphical session. If you require an interactive graphical session to Katana then you can use the X2Go client.</p> <p>Start X2Go and create a session for Katana. The details that you need to enter for the session are:</p> <pre><code>Session name: Katana\nHost: katana.restech.unsw.edu.au\nLogin: zID\nSession type: Mate\n</code></pre> <p> </p> x2go client settings <p>If you have connected from a Linux machine (or a Mac with X11 support via X11.app or XQuartz) then connecting via SSH will allow you to open graphical applications from the command line. To run these programs you should start an interactive job on one of the compute nodes so that none of the computational processing takes place on the head node.</p> <p>Warning</p> <p>The usability of a graphical connection to Katana is highly dependent on network latency and performance.</p> <p>Once you have logged into a Katana desktop, you should start a terminal </p> <p> </p> x2go desktop view <p>Then run an interactive session. Here you can see a command similar to what you would run for an interactive session with 8 CPUs and 16 GB for one hour. You will probably need more time. You can tell your interactive session has started when you see the name of the machine change - in this image I am on k247.</p> <pre><code>qsub -I -X -l select=1:ncpus=8:mem=16gb,walltime=1:00:00\n</code></pre> <p> </p> x2go terminal view <p>Once that's started, you can load the modules and run the command line name of the software you want. That is how you run Graphical Interfaces or GUIs using Katana's grunt.</p> <p> </p> x2go rstudio view <p>Putty</p> <p>MobaXTerm</p> <p>X2Go</p>"},{"location":"using_katana/github/","title":"GitHub","text":"<p>Version control systems like GitHub record changes to files over time, allowing you to go back to older versions, and create new branches for experimentation. </p> <p>Version control is most useful for keeping track of programming code and documentation - large text based corpora that aren't suitable for storing in databases.</p> <p><code>UNSW has an organisation in GitHub</code> for researchers and staff, joining this organisation gives you access to the UNSW-only repositories like:</p> <ul> <li><code>Restech-HPC - Example job scripts for Katana and NCI</code></li> <li><code>UNSW-Data-Archive - Scripts for uploading to and downloading from the UNSW Data Archive</code></li> <li><code>UNSW-eNotebook-LabArchives - UNSW eNotebook (LabArchives) widgets</code></li> </ul> <p>There is even a UNSW specific LaTeX thesis template!</p> <p>We encourage researchers to use the <code>Citation File Format</code> when writing code.</p>"},{"location":"using_katana/github/#how-to-sign-up-to-the-unsw-github-organisation","title":"How to sign up to the UNSW GitHub organisation","text":"<p>Please follow the instructions on UNSW research GitHub page.</p> <ul> <li>Restech-HPC - Example job scripts for Katana and NCI</li> <li>UNSW-Data-Archive - Scripts for uploading to and downloading from the UNSW Data Archive</li> <li>UNSW-eNotebook-LabArchives - UNSW eNotebook (LabArchives) widgets</li> <li>UNSW has an organisation in GitHub</li> <li>Citation File Format</li> <li>UNSW research GitHub page</li> </ul>"},{"location":"using_katana/ondemand/","title":"Katana OnDemand","text":"<p>There are a number of GUI based software packages that are offered through a web interface. The requirements are a Katana account and for the user to be either on campus or connected to the UNSW VPN.</p> <p>These can be accessed from https://kod.restech.unsw.edu.au</p> <p>After logging in, you can:</p> <ul> <li> <p>Access your <code>/home/</code>, <code>/srv/scratch/</code> and any project files from the Files menu. While you can upload files through this interface, please don't upload anything larger than about 10MB. </p> </li> <li> <p>See your active jobs in a web based view of the traditional <code>qstat</code> command</p> </li> <li> <p>Start a new interactive app</p> </li> </ul> <p>Installed interactive apps are availble in a drop-down list</p> <p> </p> ondemand interactive apps <p>The status of your interactive sessions can be seen using the rightmost 'My Interactive Sessions' icon. </p> <p> </p> ondemand interactive sessions <p>Clicking on the Session ID display files that are useful for debubbing purposes. Please include their contents if submitting a support request.</p>"},{"location":"using_katana/ondemand/#software-available-in-ondemand","title":"Software Available in OnDemand","text":"<ul> <li>R and RStudio</li> <li>Jupyter Notebooks, JupyterLab and various kernels</li> <li>Matlab</li> <li>Stata</li> <li>Comsol</li> <li>Ansys</li> </ul>"},{"location":"using_katana/running_jobs/","title":"Running Jobs on Katana","text":""},{"location":"using_katana/running_jobs/#brief-overview","title":"Brief Overview","text":"<p>The Login Node of a cluster is a shared resource for all users and is used for preparing, submitting and managing jobs. </p> <p>Note</p> <p>Never run any computationally intensive processes on the login nodes. </p> <p>Jobs are submitted from the login node, which delivers them to the Head Node for job and resource management. Once the resources have been allocated and are available, the job will run on one or more of the compute nodes as requested. </p> <p>Different clusters use different tools to manage resources and schedule jobs - OpenPBS_ and SLURM_ are two popular systems. Katana, like NCI's Gadi, uses OpenPBS_ for this purpose.</p> <p>Jobs are submitted using the <code>qsub</code> command. There are two types of job that <code>qsub</code> will accept: an Interactive Job and a Batch Job. Regardless of type, the resource manager will put your job in a Queue.</p> <p>An interactive job provides a shell session on a Compute Nodes. You interact directly with the compute node running the software you need explicitly. Interactive jobs are useful for experimentation, debugging, and planning for batch jobs.</p> <p>Note</p> <p>For calculations that run longer than a few hours, batch jobs are preferred.   </p> <p>In contrast, a Batch Job is a scripted job that - after submission via <code>qsub</code> - runs from start to finish without any user intervention. The vast majority of jobs on the cluster are batch jobs. This type of job is appropriate for production runs that will consume several hours or days. </p> <p>To submit a Batch Job you will need to create a job script which specifies the resources that your job requires and calls your program. The general structure of A Job Script is shown below.</p> <p>Important</p> <p>All jobs go into a Queue while waiting for resources to become available. The length of time your jobs wait in a queue for resources depends on a number of factors.</p> <p>The main resources available for use are Memory (RAM), CPU Core (number of CPUs) and Walltime (how long you want the CPUs for). These need to be considered carefully when writing your job script, since the decisions you make will impact which queue your jobs ends up on.</p> <p>As you request more memory, CPU cores, or walltime, the number of available queues goes down. The limits are which the number of queues decrease are summarised in the table below</p>"},{"location":"using_katana/running_jobs/#job-queue-limits-summary","title":"Job queue limits summary","text":"<p>Typical job queue limit cut-offs are shown below. The walltime is what determines whether a job can be run on any node, or only on a restricted set of nodes.</p> Resource Queue limit cut-offs Memory (GB) 124 180 248 370 750 1000 CPU Cores 16 20 24 28 32 44 Walltime (hrs) 12 48 100 200 Any node School-owned or general-use nodes School-owned nodes only <p>Note</p> <p>Try to combine or divide batch jobs to fit within that 12 hour limit for fastest starting times. </p> <p>The resources available on a specific compute node can be shown with the qstat command.  </p>"},{"location":"using_katana/running_jobs/#interactive-jobs","title":"Interactive Jobs","text":"<p>An interactive job or interactive session is a session on a compute node with the required physical resources for the period of time requested. To request an interactive job, add the -I flag (capital i) to <code>qsub</code>. Default sessions will have 1 CPU core, 1GB and 1 hour</p> <p>For example, the following two commands. The first provides a default session, the second provides a session with two CPU core and 8GB memory for three hours. You can tell when an interactive job has started when you see the name of the server change from <code>katana1</code> or <code>katana2</code> to the name of the server your job is running on. In these cases it's <code>k181</code> and <code>k201</code> respectively.</p> Default ResourcesCustom Resources <pre><code>[z1234567@katana1 ~]$ qsub -I\nqsub: waiting for job 313704.kman.restech.unsw.edu.au to start\nqsub: job 313704.kman.restech.unsw.edu.au ready\n[z1234567@k181 ~]$ </code></pre> <pre><code>[z1234567@katana2 ~]$ qsub -I -l select=1:ncpus=2:mem=8gb,walltime=3:00:00\nqsub: waiting for job 1234.kman.restech.unsw.edu.au to start\nqsub: job 1234.kman.restech.unsw.edu.au ready\n[z1234567@k201 ~]$ </code></pre> <p>Jobs are constrained by the resources that are requested. In the previous example the first job - running on <code>k181</code> - would be terminated after 1 hour or if a command within the session consumed more than 8GB memory. The job (and therefore the session) can also be terminated by the user with <code>CTRL-D</code> or the <code>logout</code> command.</p> <p>Interactive jobs can be particularly useful while developing and testing code for a future batch job, or performing an interactive analysis that requires significant compute resources. Never attempt such tasks on the login node -- submit an interactive job instead.</p>"},{"location":"using_katana/running_jobs/#batch-jobs","title":"Batch Jobs","text":"<p>A batch job is a script that runs autonomously on a compute node. The script must contain the necessary sequence of commands to complete a task independently of any input from the user. This section contains information about how to create and submit a batch job on Katana.</p>"},{"location":"using_katana/running_jobs/#getting-started","title":"Getting Started","text":"<p>The following script simply executes a pre-compiled program (\"myprogram\") in the user's home directory:</p> <pre><code>#!/bin/bash\n\ncd $HOME\n\n./myprogram\n</code></pre> <p>This script can be submitted to the cluster with <code>qsub</code> and it will become a job and be assigned to a queue. If the script is in a file called <code>myjob.pbs</code> then the following command will submit the job with the default resource requirements (1 CPU core with 1GB of memory for 1 hour):</p> <pre><code>[z1234567@katana1 ~]$ qsub myjob.pbs\n1237.kman.restech.unsw.edu.au\n</code></pre> <p>As with interactive jobs, the <code>-l</code> (lowercase L) flag can be used to specify resource requirements for the job:</p> <pre><code>[z1234567@katana ~]$ qsub -l select=1:ncpus=1:mem=4gb,walltime=12:00:00 myjob.pbs\n1238.kman.restech.unsw.edu.au\n</code></pre> <p>If we wanted to use the GPU resources, we would write something like this - note that because of configuration of machines, you should request: <code>ncpus=(#ngpus*8):mem=(#ngpus*46)</code></p> <pre><code>[z1234567@katana ~]$ qsub -l select=1:ncpus=8:ngpus=1:mem=46gb,walltime=12:00:00 myjob.pbs\n1238.kman.restech.unsw.edu.au\n</code></pre>"},{"location":"using_katana/running_jobs/#a-job-script","title":"A Job Script","text":"<p>Job scripts offer a much more convenient method for invoking any of the options that can be passed to <code>qsub</code> on the command-line. In a shell script, a line starting with # is a comment and will be ignored by the shell interpreter. However, in a job script, a line starting with #PBS can be used to pass options to the <code>qsub</code> command.</p> <p>Here is an overview of the different parts of a job script which we will examine further below. In the following sections we will add some code, explain what it does, then show some new code, and iterate up to something quite powerful.</p> <p>For the previous example, the job script could be rewritten as:</p> <pre><code>#!/bin/bash\n\n#PBS -l select=1:ncpus=1:mem=4gb\n#PBS -l walltime=12:00:00\n\ncd $HOME\n\n./myprogram\n</code></pre> <p>Warning</p> <p>Be careful not to use resource request formats that are old, or intended for a different cluster. E.g. nodes=X:ppn=Y is an old resource request format, not meant katana's current setup. </p> <p>This structure is the most common that you will use. The top line must be <code>#!/bin/bash</code> - we are running bash scripts, and this is required. The following section - the lines starting with <code>#PBS</code> - are where we will be configuring how the job will be run - here we are asking for resources. The final section shows the commands that will be executed in the configured session.</p> <p>The script can now be submitted with much less typing:</p> <pre><code>[z1234567@katana ~]$ qsub myjob.pbs\n1239.kman.restech.unsw.edu.au\n</code></pre> <p>Unlike submission of an interactive job, which results in a session on a compute node ready to accept commands, the submission of a batch job returns the ID of the new job. This is confirmation that the job was submitted successfully. The job is now processed by the job scheduler and resource manager. Commands for checking the status of the job can be found in the section Managing Jobs on Katana.</p> <p>If you wish to be notified by email when the job finishes then use the <code>-M</code> flag to specify the email address and the <code>-m</code> flag to declare which events cause a notification. Here we will get an email if the job aborts (<code>-m a</code>) due to an error or ends (<code>-m e</code>) naturally. </p> <pre><code>#PBS -M your.name.here@unsw.edu.au\n#PBS -m ae\n</code></pre> <p>The output that would normally go to screen and error messages of a batch job will be saved to file when your job ends. By default these files will be called <code>JOB_NAME.oJOB_ID</code> and <code>JOB_NAME.eJOB_ID</code>, and they will appear in the directory that was the current working directory when the job was submitted. In the above example, they would be <code>myjob.o1239</code> and <code>myjob.e1239</code>.  You can merge these into a single file with the <code>-j oe</code> flag. The <code>-o</code> flag allows you to rename the file.</p> <pre><code>#PBS -j oe\n#PBS -o /home/z1234567/results/Output_Report\n</code></pre> <p>When a job starts, it needs to know where to save its output and do its work. This is called the current working directory. By default the job scheduler will make your current working directory your home directory (<code>/home/z1234567</code>). This isn't likely or ideal and is important that each job sets its current working directory appropriately. There are a couple of ways to do this, the easiest is to set the current working directory to the directory you are in when you execute <code>qsub</code> by using</p> <pre><code>cd $PBS_O_WORKDIR\n</code></pre> <p>There is one last special variable you should know about, especially if you are working with large datasets. The storage on the compute node your job is running on will always be faster than the network drive.</p> <p>If you use the storage close to the CPUs - in the server rather than on the shared drives, called Local Scratch - you can often save hours of time reading and writing across the network. </p> <p>In order to do this, you can copy data to and from the local scratch, called <code>$TMPDIR</code>:</p> <pre><code>cp /home/z1234567/project/massivedata.tar.gz $TMPDIR\ntar xvf massivedata.tar.gz\nmy_analysis.py massive_data\ncp -r $TMPDIR/my_output /home/z1234567\n</code></pre> <p>There are a lot of things that can be done with PBSPro, but you don't and won't need to know it all. These few basics will get you started. </p> <p>Here's the full script as we've described. You can copy this into a text editor and once you've changed our dummy values for yours, you only need to change the last line.</p> <pre><code>#!/bin/bash\n\n#PBS -l select=1:ncpus=1:mem=4gb\n#PBS -l walltime=12:00:00\n#PBS -M your.name.here@unsw.edu.au\n#PBS -m ae\n#PBS -j oe\n#PBS -o /home/z1234567/results/Output_Report\n\ncd $PBS_O_WORKDIR\n\n./myprogram\n</code></pre>"},{"location":"using_katana/running_jobs/#array-jobs","title":"Array Jobs","text":"<p>One common use of computational clusters is to do the same thing multiple times - sometimes with slightly different input, sometimes to get averages from randomness within the process. This is made easier with array jobs.</p> <p>An array job is a single job script that spawns many almost identical sub-jobs. The only difference between the sub-jobs is an environment variable <code>$PBS_ARRAY_INDEX</code> whose value uniquely identifies an individual sub-job. A regular job becomes an array job when it uses the <code>#PBS -J</code> flag. </p> <p>For example, the following script will spawn 100 sub-jobs. Each sub-job will require one CPU core, 1GB memory and 1 hour run-time, and it will execute the same application. However, a different input file will be passed to the application within each sub-job. The first sub-job will read input data from a file called <code>1.dat</code>, the second sub-job will read input data from a file called <code>2.dat</code> and so on. </p> <p>Note</p> <p>In this example we are using <code>brace expansion</code> - the {} characters around the bash variables - because they are needed for variables that change, like array indices. They aren't strictly necessary for <code>$PBS_O_WORKDIR</code> but we include them to show consistency.</p> <pre><code>#!/bin/bash\n\n#PBS -l select=1:ncpus=1:mem=1gb\n#PBS -l walltime=1:00:00\n#PBS -j oe\n#PBS -J 1-100\n\ncd ${PBS_O_WORKDIR}\n\n./myprogram ${PBS_ARRAY_INDEX}.dat\n</code></pre> <p>There are some more examples of array jobs including how to group your computations in an array job on the UNSW Github HPC examples page.</p>"},{"location":"using_katana/running_jobs/#splitting-large-batch-jobs","title":"Splitting large Batch Jobs","text":"<p>If your batch job can be split into multiple steps you may want to split one big job up into a number of smaller jobs. There are a number of reasons to spend the time to implement this.</p> <ol> <li>If your large job runs for over 200 hours, it won't finish on Katana.</li> <li>If your job has multiple steps which use different amounts of resources at each step. If you have a pipeline that takes 50 hours to run and needs 200GB of memory for an hour, but only 50GB the rest of the time, then the memory is sitting idle. </li> <li>Katana has prioritisations based on how many resources any one user uses. If you ask for 200GB of memory, this will be accounted for when working out your next job's priority.</li> <li>Because there are many more resources for 12 hour jobs, seven or eight 12 hour jobs will often finish well before a single 100 hour job even starts. </li> </ol>"},{"location":"using_katana/running_jobs/#get-information-about-the-state-of-the-scheduler","title":"Get information about the state of the scheduler","text":"<p>When deciding which jobs to run, the scheduler takes the following details into account:</p> <ul> <li>are there available resources</li> <li>how recently has this user run jobs successfully</li> <li>how many resources has this user used recently</li> <li>how long is the job's Walltime</li> <li>how long has the job been in the queue</li> </ul> <p>You can get an overview of the compute nodes and a list of all the jobs running on each node using <code>pstat</code></p> <pre><code>[z1234567@katana2 src]$ pstat\nk001  normal-mrcbio           free          12/44   200/1007gb  314911*12\nk002  normal-mrcbio           free          40/44    56/ 377gb  314954*40\nk003  normal-mrcbio           free          40/44   375/ 377gb  314081*40\nk004  normal-mrcbio           free          40/44    62/ 377gb  314471*40\nk005  normal-ccrc             free           0/32     0/ 187gb\nk006  normal-physics          job-busy      32/32   180/ 187gb  282533*32\nk007  normal-physics          job-busy      32/32   180/ 187gb  284666*32\nk008  normal-physics          free           0/32     0/ 187gb\nk009  normal-physics          job-busy      32/32   124/ 187gb  314652*32\nk010  normal-physics          free           0/32     0/ 187gb      </code></pre> <p>To get information about a particular node, you can use <code>pbsnodes</code> but on its own it is a firehose. Using it with a particular node name is more effective:</p> <pre><code>[z1234567@katana2 src]$ pbsnodes k254\nk254\n    Mom = k254\n    ntype = PBS\n    state = job-busy\n    pcpus = 32\njobs = 313284.kman.restech.unsw.edu.au/0, 313284.kman.restech.unsw.edu.au/1, 313284.kman.restech.unsw.edu.au/2 resources_available.arch = linux\n    resources_available.cpuflags = avx,avx2,avx512bw,avx512cd,avx512dq,avx512f,avx512vl\n    resources_available.cputype = skylake-avx512\n    resources_available.host = k254\n    resources_available.mem = 196396032kb\n    resources_available.ncpus = 32\nresources_available.node_weight = 1\nresources_available.normal-all = Yes\n    resources_available.normal-qmchda = Yes\n    resources_available.normal-qmchda-maths_business-maths = Yes\n    resources_available.normal-qmchda-maths_business-maths-general = Yes\n    resources_available.vmem = 198426624kb\n    resources_available.vnode = k254\n    resources_available.vntype = compute\n    resources_assigned.accelerator_memory = 0kb\n    resources_assigned.hbmem = 0kb\n    resources_assigned.mem = 50331648kb\n    resources_assigned.naccelerators = 0\nresources_assigned.ncpus = 32\nresources_assigned.ngpus = 0\nresources_assigned.vmem = 0kb\n    resv_enable = True\n    sharing = default_shared\n    last_state_change_time = Thu Apr 30 08:06:23 2020\nlast_used_time = Thu Apr 30 07:08:25 2020\n</code></pre>"},{"location":"using_katana/running_jobs/#managing-jobs-on-katana","title":"Managing Jobs on Katana","text":"<p>Once you have jobs running, you will want visibility of the system so that you can manage them - delete jobs, change jobs, check that jobs are still running.</p> <p>There are a couple of easy to use commands that help with this process.</p> <p>Job Commands</p> qstatqdelqalter <p>Show all jobs on the system <code>qstat</code> gives very long output. Consider piping to <code>less</code></p> <pre><code>    [z1234567@katana2 ~]$ qstat | less\n    Job id            Name             User              Time Use S Queue\n    ----------------  ---------------- ----------------  -------- - -----\n    245821.kman       s-m20-i20-200h   z1234567                 0 Q medicine200\n    280163.kman       Magcomp25A2      z1234567          3876:18: R mech700\n    282533.kman       Proj_MF_Nu1      z1234567          3280:08: R cosmo200\n    284666.kman       Proj_BR_Nu1      z1234567          3279:27: R cosmo200\n    308559.kman       JASASec55        z1234567          191:21:3 R maths200\n    309615.kman       2020-04-06.BUSC  z1234567          185:00:5 R babs200\n    310623.kman       Miaocyclegan     z1234567          188:06:3 R simigpu200\n    ...\n</code></pre> <p>List just my jobs</p> <p>You can use either your ZID or the Environment Variable <code>$USER</code></p> <pre><code>    [z2134567@katana2 src]$ qstat -u $USER\nkman.restech.unsw.edu.au: Req'd  Req'd   Elap\n    Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n    --------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n    315230.kman.res z2134567 general1 job.pbs       --    1   1    1gb 01:00 Q   -- </code></pre> <p>If you add the <code>-s</code> flag, you will get slightly more status information.</p> <pre><code>[z1234567@katana2 src]$ qstat -su z1234567\n\nkman.restech.unsw.edu.au: Req'd  Req'd   Elap\nJob ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n315230.kman.res z1234567 general1 job.pbs     61915   1   1    1gb 01:00 R 00:03\nJob run at Fri May 01 at 14:28 on (k019:mem=1048576kb:ncpus=1:ngpus=0)\n315233.kman.res z1234567 general1 job.pbs       --    1   1    1gb 01:00 Q   --\n    -- </code></pre> <p>List information about a particular job <pre><code>[z1234567@katana2 src]$ qstat -f 315236                                                                                                                                       \nJob Id: 315236.kman.restech.unsw.edu.au                                                                                                                                       Job_Name = job.pbs                                                                                                                                                        Job_Owner = z1234567@katana2\n    job_state = Q\n    queue = general12\n    server = kman.gen\n    Checkpoint = u\n    ctime = Fri May  1 14:41:00 2020\nError_Path = katana2:/home/z1234567/src/job.pbs.e315236\n    group_list = GENERAL\n    Hold_Types = n\n    Join_Path = n\n    Keep_Files = n\n    Mail_Points = a\n    mtime = Fri May  1 14:41:00 2020\nOutput_Path = katana2:/home/z1234567/src/job.pbs.o315236\n    Priority = 0\nqtime = Fri May  1 14:41:00 2020\nRerunable = True\n    Resource_List.ib = no\n    Resource_List.mem = 1gb\n    Resource_List.ncpus = 1\nResource_List.ngpus = 0\nResource_List.nodect = 1\nResource_List.place = pack\n    Resource_List.select = 1:mem=1gb:ncpus=1\nResource_List.walltime = 01:00:00\n    substate = 10\nVariable_List = PBS_O_HOME=/home/z1234567,PBS_O_LANG=en_AU.UTF-8,\n        PBS_O_LOGNAME=z1234567,\n        PBS_O_PATH=/home/z1234567/bin:/usr/lib64/qt-3.3/bin:/usr/lib64/ccache:\n        /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/pbs/bin,PBS_O_M\n        AIL=/var/spool/mail/z1234567,PBS_O_SHELL=/bin/bash,PBS_O_WORKDIR=/home\n        /z1234567/src,PBS_O_SYSTEM=Linux,PBS_O_QUEUE=submission,PBS_O_HOST=kat\n        ana2\n    etime = Fri May  1 14:41:00 2020\neligible_time = 00:00:00\n    Submit_arguments = -W group_list=GENERAL -N job.pbs job.pbs.JAZDNgL\n    project = _pbs_project_default\n</code></pre></p> <p>Remove a job from the queue or kill it if it's started. To remove an array job, you must include the square braces and they will need to be escaped. In that situation you use <code>qdel 12345\\[\\]</code>. Uses the <code>$JOBID</code> </p> <pre><code>[z1234567@katana2 src]$ qdel 315252\n</code></pre> <p>Once a job has been submitted, it can be altered. However, once a job begins execution, the only values that can be modified are <code>cputime</code>, <code>walltime</code>, and <code>run_count</code>. These can only be reduced.</p> <p>Users can only lower resource requests on queued jobs. If you need to increase resources, contact a systems administrator. In this example you will see the resources change - but not the <code>Submit_arguments</code></p> <pre><code>[z1234567@katana2 src]$ qsub -l select=1:ncpus=2:mem=128mb job.pbs\n315259.kman.restech.unsw.edu.au\n[z1234567@katana2 src]$ qstat -f 315259\nJob Id: 315259.kman.restech.unsw.edu.au\n    ...\n    Resource_List.mem = 128mb\n    Resource_List.ncpus = 2\n...\n    Submit_arguments = -W group_list=GENERAL -N job.pbs -l select=1:ncpus=2:mem=128mb job.pbs.YOOu3lB\n    project = _pbs_project_default\n\n[z1234567@katana2 src]$ qalter -l select=1:ncpus=4:mem=512mb 315259; qstat -f 315259\nJob Id: 315259.kman.restech.unsw.edu.au\n    ...\n    Resource_List.mem = 512mb\n    Resource_List.ncpus = 4\n...\n    Submit_arguments = -W group_list=GENERAL -N job.pbs -l select=1:ncpus=2:mem=128mb job.pbs.YOOu3lB\n    project = _pbs_project_default\n</code></pre>"},{"location":"using_katana/running_jobs/#tips-for-using-pbs-and-katana-effectively","title":"Tips for using PBS and Katana effectively","text":""},{"location":"using_katana/running_jobs/#keep-your-jobs-under-12-hours-if-possible","title":"Keep your jobs under 12 hours if possible","text":"<p>If you request more than 12 hours of <code>WALLTIME</code> then you can only use the nodes bought by your school or research group. Keeping your job's run time request under 12 hours means that it can run on any node in the cluster.</p> <p>Important</p> <p>Two 10 hour jobs will probably finish sooner that one 20 hour job.</p> <p>In fact, if there is spare capacity on Katana, which there is most of the time, six 10 hours jobs will finish before a single 20 hour job will. Requesting more resources for your job decreases the places that the job can run</p> <p>The most obvious example is going over the 12 hour limit which limits the number of compute nodes that your job can run on. For example specifying the CPU in your job script restricts you to the nodes with that CPU. A job that requests 20Gb will run on a 128Gb node with a 100Gb job already running but a 30Gb job will not be able to.</p>"},{"location":"using_katana/running_jobs/#running-your-jobs-interactively-makes-it-hard-to-manage-multiple-concurrent-jobs","title":"Running your jobs interactively makes it hard to manage multiple concurrent jobs","text":"<p>If you are currently only running jobs interactively then you should move to batch jobs which allow you to submit more jobs which then start, run and finish automatically. If you have multiple batch jobs that are almost identical then you should consider using array jobs</p> <p>If your batch jobs are the same except for a change in file name or another variable then you should have a look at using array jobs.</p>"}]}